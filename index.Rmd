---
title: <span style="font-size:125%; font-family:'Questrial'; font-style:normal"><img src='img/198R.png' style="display:inline; vertical-align:bottom; height:1.25em; width:1.25em" ></img> Models</span>
subtitle: <span style="font-size:125%; font-style:normal; font-variant:small-caps; font-family:'Questrial'">Quick Reference</span>
author:  |
  <span style="font-size:125%; font-style:normal; font-family:'Questrial'">Michael Clark</span> <br>
  <span class="" style="font-size:75%">https://m-clark.github.io</span><br><br>
output:
  html_document:
    css: [css/standard_html.css]
    number_sections: false
    df_print: kable
    fig_caption: yes
    highlight: pygments
    theme: sandstone
    toc: true
    toc_depth: 2
    toc_float:
      collapse: section
      smooth_scroll: false    
      scroll_highlight: yes
bibliography: refs.bib
biblio-style: apalike
link-citations: yes
output_dir: "docs"
description: ""    
font-import: https://fonts.googleapis.com/css?family=Roboto|Roboto+Mono|Open+Sans|Alex+Brush|Questrial|Stalemate
cover-image: img/nineteeneightyR.png
url: 'https\://m-clark.github.io/' 
favicon: 'img/R.ico'
github-repo:  m-clark/
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = T, eval=F, message=F, warning=F, error=F, comment=NA, R.options=list(width=220),   # code 
                      dev.args=list(bg = 'transparent'), dev='svglite',                                 # viz
                      fig.align='center', out.width='75%', fig.asp=.75,                 
                      cache.rebuild=F, cache=T)                                                         # cache
```

```{r packages, include=FALSE, cache=FALSE, eval=TRUE}
library(magrittr); library(tidyverse); library(stringr); library(pander); 
library(plotly); library(lazerhawk); library(viridis); library(visNetwork)
```



```{r rdoc_api, echo=FALSE, eval=TRUE}
# Function to get package percentile rank
get_perc = function(pack) {
  require(httr)
  url = paste0('http://rdocumentation.org/api/packages/', pack, '/percentile')
  out = GET(url) %>% 
    content()
  out$percentile
}
get_depgraph = function(pack) {
  require(httr)
  url = paste0('http://rdocumentation.org/api/packages/', pack, '/reversedependencies')
  out = GET(url) %>% 
    content()
  node_df = data.table::rbindlist(out$nodes, fill = T)
  edge_df = data.table::rbindlist(out$links, fill = T)
  list(node_df=node_df, edge_df=edge_df)
}
```

THIS DOCUMENT IS A WORK IN PROGRESS. IT IS CURRENTLY NOT EVEN HALF-WAY THROUGH.

# Introduction

This is a quick reference for the modeling syntax and associated packages.  While it covers a lot of ground, it is not meant to be exhaustive, but rather, it provides an easy reference for those new to R, someone trying out an unfamiliar (but otherwise common) technique, or those just interested in comparison to similar approaches in other environments.  In time, this may be rendered obsolete by the [parsnip package](https://github.com/topepo/parsnip), but until then, this can get you quickly started with many common models.

Preference is given to base R functionality or what are perceived as very commonly used packages.  'Very common' is based on my own knowledge consulting across dozens of disciplines along with just regularly following the R community, and things like the rankings at [RDocumentation.org](https://rdocumentation.org), and packages listed in [CRAN Task Views](https://www.rdocumentation.org/taskviews).


#### Miscellaneous

I will ignore the data argument in every example that uses the typical R formula approach. It is implied that you would supply it. 

For those who want to get more into the implementation of many of these models, I have raw R code that demonstrates many (probably most) of the techniques discussed [here](https://github.com/m-clark/Miscellaneous-R-Code).

What this won't provide:

- data processing required
- a detailed explanation of the model and why you'd use it
- visualization
- a list of every package that *might* be useful

For some content I will provide recommended reading.


Color coding:

- <span class="emph">emphasis</span>
- [link]()
- <span class="pack">package</span>
- <span class="basepack">package that comes with standard R installation</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>

When you see `pack::func` the first value is the package name and the second the function.  I show this for the first time to make clear what package is used, but otherwise it is assumed you've loaded the necessary package (`library(mypackage)`).


# Basics

Most packages follow the same basic modeling syntax, and those who don't generally suffer less usage.  Just specify your target variable, say `y` and your predictor variables, e.g. `x`.  For some packages you may have to instead supply a model.matrix `X` and separate `y` vector (with no data argument).


## Standard modeling

```{r basics-standard-modeling}
model_func(y ~ x, data=mydata)
model_func(X, y)
model_func(X)  # unsupervised (e.g. principal components analysis)
```

Some models may be different enough to warrant their own syntax, e.g. latent variable models/SEM.  However, even then, packages try to adhere to the `y ~ x` formula approach.

## Exploring Results

Most modeling functions, again, those that actually want to be used, will provide the following commonly used methods.

```{r basics-explore}
summary(my_model)    # may also be just print()
fitted(my_model)     # fitted values
residuals(my_model)  # residuals
predict(my_model, newdata=other_observations, type='response')  # specific predictions
```

Some may also have a <span class="func">plot</span> method, but this will definitely vary by package both in whether it is available and what it will do.  For example, plotting an <span class="objclass">lm</span> object will explore residuals, while plotting a <span class="objclass">gam</span> will show the component plots of the smoothed terms. Some packages, like <span class="pack">ggeffects</span>, <span class="pack">visreg</span> and <span class="pack">margins</span>, will help plotting some types of effects.  I've found it easier to do oneself via <span class="pack">ggplot2</span>, rather than hope you can finagle a plot made by a package into something you actually want.

Many packages will also have a <span class="func">confint</span> method for interval estimates of coefficients.  See the <span class="basepack">boot</span> package as a starting point for bootstrapped intervals.

## Comparing models

Good science will need competing models.  For many models, one can use a likelihood ratio test or something like AIC.

```{r model_comparison}
anova(mod)
anova(mod1, mod2)

AIC(mod)
AIC(mod1, mod2)
```

Others will be package specific. For example, <span class="basepack">mgcv</span> will supply a GCV (generalized cross-validation) metric, and there is WAIC and loo in <span class="emph">Stan</span>-related packages for Bayesian analysis.

# Linear models and GLM 

Starting with the basics, we can obtain linear models (OLS) or generalized linear models as follows.

```{r glm}
aov(y ~ x + Error(id))                    # ANOVA with error stratum (e.g. repeated measures for each id)
lm(y ~ x + z)                             # standard linear model/OLS
glm(y ~ x + z, family = 'binomial')       # logistic regression with binary response
glm(y ~ x + z + offset(log(q)), family = 'poisson')  # count/rate model
```

See also, <span class="pack">rms</span>::<span class="func">ols</span> for the standard model with perks, along with <span class="func">lrm</span> for logistic regression.


## Extensions

### Interactions and Variable transformations

Unlike some statistical packages that haven't figured out what year it is, you do not have to make explicit interaction terms.  There are two ways you can do it.

```{r glm-interactions}
lm(y ~ x*z)
lm(y ~ x + z + x:z)  # equivalent
```

There are better approaches to get at nonlinear relationships than with polynomials (variables interacting with themselves), but if interested, one can use <span class="func">poly</span>.  There is also <span class="func">nls</span> for nonlinear least squares.  However, it is rare that one knows the functional form beforehand.  Just because you can fit a quadratic function or logistic growth curve model doesn't mean it's actually the right or best functional form.

```{r glm-polynomial}
lm(y ~ poly(x, degree=3))
nls(y ~ SSlogis(log(x), Asym, xmid, scal))  # see ?SSlogis for details
```

You typically can transform a variable within the formula itself.

```{r glm-transformations}
lm(y ~ log(x))
```

This is fine for quick exploration, but generally it's poor practice to do so (you'll likely need the transformed variable in other contexts), and it simply won't work with some modeling functions.

### Categorical variable

#### Predictors

For categorical predictors, most modeling packages will automatically dummy code any categorical variables.  But you can specify other coding schemes as well.

```{r glm-coding}
lm(y ~ x)  # x is a factor
?contrasts
```


#### Ordinal target

Options thin out quickly when you have more than 2 categories for your target variable.  But there is still plenty to play with in R for ordinal and multinomial models.

For ordinal models, the <span class="pack">rms</span> package is a very good starting point. The <span class="pack">ordinal</span> package will add random effects as well, in the familiar <span class="pack">lme4</span> style.

```{r glm-ordinal}
rms::orm(y ~ x)  # y is ordinal
ordinal::clmm(y ~ x + (1|g1) + (1|g2), link = "probit", threshold = "equidistant")
```


#### Multinomial

For nominal dependent variables, check out the <span class="pack">mlogit</span> package.  You will almost certainly have some data processing to do beforehand.

```{r glm-multinomial}
# x: is an generic covariate
# q: is an individual specific covariate
# z: is an alternative specific covariate
mlogit::mlogit(y ~ x|q|z)

?mlogit::mlogit.data
?mlogit::mFormula
```

See <span class="pack">mnlogit</span> for a faster approach. Technically the base R package <span class="basepack">nnet</span> will do these models too, but you won't be able to do much with the result.  

### Other distributions

The standard GLM will only take you so far in terms of distributions for the target variable.  To start your journey beyond the exponential family, you might consider the following.  Some packages will provide additional distributions in similar contexts, and some might be geared entirely toward a specific family.

- <span class="pack">glm.nb</span>: Base R function for the negative binomial
- <span class="pack">pscl</span>: Zero-inflated and hurdle models
- <span class="pack">betareg</span>, <span class="pack">dirichletReg</span>: Beta (0, 1) and Dirichlet distributions (e.g. compositional data)
- <span class="pack">VGAM</span>, <span class="pack">mgcv</span>, <span class="pack">gamlss</span>: Additive modeling packages that come with many additional distributions (e.g. student T, bivariate logit/probit, censored normal, tweedie, multivariate gaussian, etc.)

### Miscellaneous

Some might be interested in quantile regression.  The <span class="pack">quantreg</span> package would be a starting point.  In addition, the <span class="pack">car</span> package has a few miscellaneous functions for various models, e.g. add-variable plots, variance inflation factors, influence metrics, and more.

## Linear models and GLM list of packages

```{r lm_percs, eval=TRUE, echo=FALSE}
# roughly in order
rms_perc = get_perc('rms')
ordinal_perc = get_perc('ordinal')
mlogit_perc = get_perc('mlogit')
mnlogit_perc = get_perc('mnlogit')
nnet_perc = get_perc('nnet')
mgcv_perc = get_perc('mgcv')
VGAM_perc = get_perc('VGAM')
gamlss_perc = get_perc('gamlss')
pscl_perc = get_perc('pscl')
betareg_perc = get_perc('betareg')
dirichletReg_perc = get_perc('dirichletReg')
quantreg_perc = get_perc('quantreg')
car_perc = get_perc('car')
```


The following is a list of the packages mentioned in this section and their percentile rank on RDocumentation.

- <span class="pack">rms</span> `r rms_perc`%
- <span class="pack">ordinal</span> `r ordinal_perc`%
- <span class="pack">mlogit</span> `r mlogit_perc`%
- <span class="pack">mnlogit</span> `r mnlogit_perc`%
- <span class="basepack">nnet</span> `r nnet_perc`%
- <span class="basepack">mgcv</span> `r mgcv_perc`%
- <span class="pack">VGAM</span> `r VGAM_perc`%
- <span class="pack">gamlss</span> `r gamlss_perc`%
- <span class="pack">pscl</span> `r pscl_perc`%
- <span class="pack">betareg</span> `r betareg_perc`%
- <span class="pack">dirichletReg</span> `r dirichletReg_perc`%
- <span class="pack">quantreg</span> `r quantreg_perc`%
- <span class="pack">car</span> `r car_perc`%




# Regularization

Not enough people use <span class="emph">regularization</span> in their models to guard against overfitting, and it should be the default approach in my opinion, especially in small data/complex modeling scenarios.  The <span class="pack">glmnet</span> package fits a variety of models, with <span class="emph">lasso</span>, <span class="emph">ridge</span>, or something in between for a penalty.  Not a very user friendly package though.

```{r reg-glmnet}
glmnet::glmnet(X, y)
```


See also, <span class="pack">elasticnet</span>.  Better yet, go [Bayesian][Bayesian].  Note also that there are actually quite a few packages that will accept a penalty parameter/matrix, but odds are slim you'd know that in advance. 

## Regularization list of packages

```{r reg_percs, eval=TRUE, echo=FALSE}
glmnet_perc = get_perc('glmnet')
elasticnet_perc = get_perc('elasticnet')
```


- <span class="pack">glmnet</span> `r glmnet_perc`%
- <span class="pack">elasticnet</span> `r elasticnet_perc`%

# Mixed models

One of the most common generalizations from the GLM setting regards data dependent situations where observations are correlated with one another due to inherent clustering.  This is most commonly dealt with via <span class="emph">mixed models</span>.  You can find detailed examples in my document [here](https://m-clark.github.io/mixed-models-with-R).

## nlme

The <span class="pack">nlme</span> package  comes with base R.  It does standard linear and nonlinear (in the predictors) mixed models. It can add correlation structure (e.g. temporal (e.g. <span class="func">corAR</span>) and spatial (e.g. <span class="func">corGaus</span>)), as well as heterogeneous variances (e.g. <span class="func">varIdent</span>). It can also do nonlinear models of specific functional forms (e.g. <span class="func">SSlogis</span>).  However, it cannot generalize beyond the gaussian distribution.

```{r nlme}
nlme::lme(y ~ x, random = ~ 1|group)       # basic model
lme(y ~ x, random = ~ 1 + x|group)         # add random slopes
lme(y ~ x, random = list(~ 1|g1, ~ 1|g2))  # different random effects
```


In addition, one can get at additional correlation/variance structure, though it may not be very intuitive.  The first line examines heterogeneous variances across some grouping factor `q`.  The other demonstrates an autoregressive correlation structure.

```{r nlme_cor}
lme(y ~ x, random = ~ 1|group, weights = varIdent(form=~1|q))
lme(y ~ x, random = ~ 1|group, correlation = corAR1(form=~1|q))
```


## lme4

The lme4 package is one of the most widely used modeling packages in R.  It's the best tool for mixed models, and I've used many within R and beyond.  It doesn't do everything, but it does a lot, and fast.  It serves most people's needs just fine.  Beyond that, many other packages work or depend on it, and other packages that extend lme4 will use the same modeling syntax.

In contrast to nlme, <span class="pack">lme4</span> is very efficient but does not have the capability (in a practical way) to do things like further examination of residual structure.  It does however extend the models to the GLM family (and negative binomial with glmer.nb).

```{r lme4}
lme4::lmer(y ~ x + (1|group))
lmer(y ~ x + (1|g1) + (1|g2))
lmer(y ~ x + (1 + x|group))
glmer(y ~ x + (1|group), family = 'binomial')
```

Note that if you're labeling your data correctly, it doesn't matter to lme4 whether the clustering is nested or crossed.  However if you give the nested clusters the same labels (e.g. 1,2,3,4 in every A, B, C), you'll need to note this.

```{r lme4_nest}
lmer(y ~ x + (1|group/nested_group))
lmer(y ~ x + (1|group) + (1|group:nested_group))
```

Again though, you shouldn't label your data such that the same label can refer to multiple things.




### lme4 extensions

The following is a graph of packages that have a connection to lme4 directly (thick lines) or indirectly, and gives a sense that you'll have a lot to work with should you use this package.  Zoom in to see package names.

```{r lme4_getgraph, eval=TRUE, echo=FALSE}
lme4_depgraph = get_depgraph('lme4')
```

```{r lme4_graph, echo=F, eval=T}
nodes = lme4_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = lme4_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

adj_mat = create_adjacency(edges, n1 = 'from', n2='to')

library(igraph)
deg = degree(graph.adjacency(adj_mat))
deg = data.frame(id = as.integer(names(deg))-1,  # for some reason something is zero
                 value=deg)
nodes = left_join(nodes, deg) %>% 
  mutate(value = ifelse(is.na(value), min(value, na.rm = T), value),
         value = value-min(value)+1)


visNetwork(nodes = nodes, edges=edges)
```


You'll note in the graph one of the relatively large nodes regards the car package. People use that because lme4 won't give you p-values.  You don't need them though, because you can just use confint to get the interval estimates, e.g. via bootstrap.  The merTools package provides a more efficient approach to interval estimation, and in my experience is more like what you'd get with a Bayesian approach.

Here are some other useful packages:

<span class="pack">merTools</span>: extracting useful information from and exploring the implications of <span class="objclass">merMod</span> objects 
<span class="pack">lmertest</span>: likelihood ratio and other tests
<span class="pack">flexmix</span>: mixture mixed models (e.g. latent class growth/trajectory)
<span class="pack">mediation</span>: mediation models with <span class="objclass">merMod</span> objects
<span class="basepack">mgcv</span>: additive mixed models (also via <span class="basepack">nlme</span>)
<span class="pack">brms</span>: Bayesian approach with the same syntax

## The mixed model object

Several mixed model packages have the same methods to extract certain features.

```{r mm_objects}
fixef(model)       # fixed effects coefficient
ranef(model)       # random effects coefficient
VarCorr(model)     # variance components
```


## Related models

You won't find much in the way of so-called <span class="emph">fixed effects models</span> (for panel data) in R, but the <span class="pack">plm</span> package does this .  The <span class="pack">geepack</span> will allow for <span class="emph">GEE</span> models to analyse data with various types of covariance structures.  And if all you want to do is correct your standard errors ('cluster-robust'), consider <span class="pack">sandwich</span>.

## Still other packages

We've mentioned <span class="pack">mgcv</span> (more detail later) and <span class="pack">ordinal.</span> One of the <span class="pack">lme4</span> developers is extending things to more complex models with <span class="pack">glmmTMB</span>.

## Mixed models list of packages

I have [several documents](http://m-clark.github.io/documents/#mixed-models) demonstrating mixed models for further exploration.


```{r mixed_percs, eval=TRUE, echo=FALSE}
nlme_perc = get_perc('nlme')
lme4_perc = get_perc('lme4')
plm_perc = get_perc('plm')
geepack_perc = get_perc('geepack')
sandwich_perc = get_perc('sandwich')
mgcv_perc = get_perc('mgcv')
ordinal_perc = get_perc('ordinal')
```


- <span class="basepack">nlme</span> `r nlme_perc`%
- <span class="pack">lme4</span> `r lme4_perc`%
- <span class="pack">plm</span> `r plm_perc`%
- <span class="pack">geepack</span> `r geepack_perc`%
- <span class="pack">sandwich</span> `r sandwich_perc`%
- <span class="pack">ordinal</span> `r ordinal_perc`%
- <span class="pack">mgcv</span> `r mgcv_perc`%

## Mixed models recommended reading

@gelman_arm

@wood_generalized_2017

@fahrmeir2013regression

# Additive models

<span class="emph">Generalized additive models</span> incorporate nonlinear, random, and spatial effects all under one roof.  One of the most powerful modeling packages in the R universe, the excellently named Mixed-GAM Computational Vehicle (<span class="pack">mgcv</span>), focuses on these, and even comes with the standard R installation!  

My own opinion is that GAMs should be one's baseline model, as they allow for more complex modeling but penalize that complexity to help guard against overfitting.  Furthermore, assuming a linear relationship for everything is overly simplistic at best.  GAMs also have close connections to mixed models, so can be seen as an extension of those.  You can find detailed examples of GAMs in my document [here](https://m-clark.github.io/generalized-additive-models).

```{r gam}
mgcv::gam(y ~ s(x))
gam(y ~ s(x), family = 'nb')                      # different family (negative binomial)
gam(y ~ s(x, k=20))                               # allow for more wiggles!
gam(y ~ s(x, by='group'))                         # different wiggles per group
gam(y ~ s(x, z))                                  # interaction
gam(y ~ s(x, bs='cc'))                            # alternate spline (cyclic cubic)
gam(y ~ s(group, bs='re'))                        # random effect with factor variable
gam(y ~ s(area, bs='mrf', xt=neighborhood_list))  # discrete spatial random effect
```

Once you run the model, you'll want to explore it.

```{r gam-explore}
gam.check(model)    # Did I let it get wiggly enough?
plot(model)         # Visualize effects
concurvity(model)   # the GAM form of collinearity
```



The package also comes with two means of modeling mixed effect models directly, with <span class="pack">nlme</span> support via the <span class="func">gamm</span> function, and the with <span class="pack">lme4</span> support via the <span class="pack">gamm4</span> package.  With the former one can also get at temporal autocorrelation as well via the `correlation` argument.  There is capability to deal with missing values, multivariate outcomes, and handle large data sets. If you don't use smooth terms, <span class="pack">mgcv</span> can simply be seen as a means to model additional distributional families.  


## Extensions and other considerations

It took a while, but people have finally started to realize the power of mgcv.

```{r mgcv_getgraph, eval=TRUE, echo=FALSE}
mgcv_depgraph = get_depgraph('mgcv')
```

```{r mgcv_graph, echo=F, eval=T}
# htmlwidgets fails with second graph. will display mixed model one but not this.
nodes = mgcv_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = mgcv_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

edge_trim = edges %>% 
  filter(to == 0)
visNetwork(nodes = nodes %>% 
             filter(id %in% c(0, edge_trim$from)) %>% 
             mutate(value=c(100, rep(0, nrow(.)-1))), 
           edges=edge_trim) %>% 
  visEdges(color=list(opacity=.10))

# adj_mat = create_adjacency(edges, n1 = 'from', n2='to')
# 
# library(igraph)
# deg = degree(graph.adjacency(adj_mat))
# deg = data.frame(id = as.integer(names(deg))-1,  # for some reason something is zero
#                  value=deg)
# nodes = left_join(nodes, deg) %>% 
#   mutate(value = ifelse(is.na(value), min(value, na.rm = T), value),
#          value = value-min(value)+1)
# 
# 
# visNetwork(nodes = nodes, edges=edges)
```



For example, most of the R community uses <span class="pack">ggplot2</span> to explore their data visually.  The package comes with <span class="func">geom_smooth</span>, which can actually use <span class="pack">mgcv</span> to display a nonlinear relationship between two variables.  Unfortunately it is only triggered automatically for large samples (> 1000), but you can always use it if desired via the `method` argument. You can even use the various arguments for the smooth term function.

```{r gam-ggplot}
ggplot2::ggplot(aes(x, y)) +
  geom_smooth(method = 'gam')

ggplot(aes(x, y)) +
  geom_smooth(method = 'gam', formula = y ~ s(x, bs='gp'))
```

I'll speak more about the package in the Bayesian section, but you can also use the <span class="func">s</span> function when using <span class="pack">brms</span>[^jagam].  Honestly, between those two packages and their respective capabilities, you'll likely need little more for your statistical modeling.  I personally use them all the time.

At one time both <span class="pack">gamlss</span> and <span class="pack">VGAM</span> extended GAMs beyond what <span class="pack">mgcv</span> could do, and they still have some modeling capabilities not seen in <span class="pack">mgcv</span>, e.g. other distributional families.  However, <span class="pack">mgcv</span> has since made up some of the more important differences.  Also, I still see some using the R-core <span class="func">spline</span> function and <span class="pack">splines</span> package.  There is nothing to be gained there however.  There is also the <span class="pack">gam</span> package, by Trevor Hastie, the person who wrote one of the most widely cited works on the technique.  While it certainly does the job, it's not on the level of <span class="pack">mgcv</span> in terms of what all it can do.  Unfortunately some packages that use GAMs use it instead of <span class="pack">mgcv</span>, so you won't have all the nice functionality.

In my document on GAMs I quote Shalizi who I think sums up the reason to use the models nicely. So I'll do so again here.

> With modern computing power, there are very few situations in which it is actually better to do linear regression than to fit an additive model. In fact, there seem to be only two good reasons to prefer linear models.<br> <br>
Our data analysis is guided by a credible scientific theory which asserts linear relationships among the variables we measure (not others, for which our observables serve as imperfect proxies).<br> <br>
Our data set is so massive that either the extra processing time, or the extra computer memory, needed to fit and store an additive rather than a linear model is prohibitive.<br> <br>
Even when the first reason applies, and we have good reasons to believe a linear theory, the truly scientific thing to do would be to check linearity, by fitting a flexible non-linear model and seeing if it looks close to linear. Even when the second reason applies, we would like to know how much bias we’re introducing by using linear predictors, which we could do by randomly selecting a subset of the data which is small enough for us to manage, and fitting an additive model. <br> <br>
In the vast majority of cases when users of statistical software fit linear models, neither of these justifications applies: theory doesn’t tell us to expect linearity, and our machines don’t compel us to use it. Linear regression is then employed for no better reason than that users know how to type lm but not gam. You now know better, and can spread the word.


```{r gam_percs, eval=TRUE, echo=FALSE}
splines_perc = get_perc('splines')
gamlss_perc = get_perc('gamlss')
VGAM_perc = get_perc('VGAM')
gam_perc = get_perc('gam')
splines_perc = get_perc('splines')
```


## Additive models list of packages

- <span class="pack">mgcv</span> `r mgcv_perc`%
- <span class="pack">gam</span> `r gam_perc`%
- <span class="pack">gamlss</span> `r gamlss_perc`%
- <span class="pack">VGAM</span> `r VGAM_perc`%
- <span class="pack">splines</span>  `r splines_perc`%

## GAM recommended reading

@wood_generalized_2017

@fahrmeir2013regression

# Survival analysis

<span class="emph">Survival analysis</span> is very common in biostatistics, epidemiology, public health etc.   It is also commonly called <span class="emph">event-history</span> analysis and <span class="emph">failure analysis</span> (engineering).    The basic idea is that you have a time-based target variable and want to model the time it takes until the event of interest occurs.  


Given the nature of the data you need two things to specify a target variable, the time counter and the indicator for whether the event of interest happened or not.

```{r survival_demo}
y = survival::Surv(time = t, event = q)  # standard right censored
y = Surv(t, t>0, type='left')            # left-censored
survival::coxph(y ~ x)                   # Cox Proportional Hazards Regression
cox.zph(model)                           # test proportional hazards assumption
anova(model)                             # anova summary table
coxph(y ~ x + strata(group))             # stratified
coxph(y ~ x + frailty.gaussian(group))   # random effect for group
coxph(Surv(start, stop, event) ~ x)      # time-dependent model
survreg(y ~ x, dist="exponential")       # parametric model
```


I've mentioned the <span class="pack">rms</span> package before.  It is written by Frank Harrell, a noted biostatistician who also contributed to SAS back in the day.  The package name is an acronym for the title of his book *Regression Modeling Strategies*, and given his discipline, he devotes a lot of content in the text, and functionality in the package, for survival analysis.  The book is a very good modeling book in general.  

```{r survival_rms}
y = survival::Surv(time = t, event = status)
dd = rms::datadist(x, z)             # set data  up for prediction
options(datadist='dd')
cph(y ~ x + z)                       # basic cox
psm(y ~ x + z, dist="weibull")       # parametric survival model
Mean(model)                          # create a function for further exploration
Survival(model)                      # create a function for further exploration
Quantile(model)                      # create a function for further exploration
ggplot(Predict(model, x, z))         # prediction plot
survplot(model)                      # survival plot
nomogram(model)                      # yep, nomogram
```

Along with the extensions that rms provides, you might also find something of use in packages like <span class="pack">Epi</span>, <span class="pack">epitools</span>, or <span class="pack">epiR</span>.  In addition, while the biostats world seems high on splines for their survival models (see e.g. `?rms::rcs`), and so you can incorporate them easily with the models above, likewise some of the additive models packages like <span class="pack">mgcv</span> have functionality for survival analysis.

```{r survival_gam}
mgcv::gam(time ~ s(x), weights = status, family = cox.ph)
```

And finally, you can find survival models in the machine learning context, e.g. with a package like <span class="pack">randomForestSRC</span>.

## Survival analysis list of packages

[Survival Task View](https://www.rdocumentation.org/taskviews#Survival)

```{r survival_percs, eval=TRUE, echo=FALSE}
survival_perc = get_perc('survival')
rms_perc = get_perc('rms')
randomForestSRC_perc = get_perc('randomForestSRC')
Epi_perc = get_perc('Epi')
epitools_perc = get_perc('epitools')
epiR_perc = get_perc('epiR')
```

- <span class="basepack">survival</span> `r survival_perc`%
- <span class="pack">rms</span> `r rms_perc`%
- <span class="pack">randomForestSRC</span>  `r randomForestSRC_perc`%
- <span class="pack">Epi</span>  `r Epi_perc`%
- <span class="pack">epitools</span>  `r epitools_perc`%
- <span class="pack">epiR</span>  `r epiR_perc`%
- <span class="pack">mgcv</span> `r mgcv_perc`%


## Survival analysis recommended reading

@harrell2015

# Survey weighting

Many data sets in the social sciences are the result of a <span class="emph">survey design</span>, where certain geographic regions and populations are sampled in precise ways.  With the survey weights, one can make more appropriate inferences to the population from which the data was drawn.

I don't have a lot of experience here, but can at least show a demo and provide some resources.  The first issue is setting up the survey design.  Your data will presumably have the weights and other necessary information (e.g. the finite population correction).

```{r survey_glm}
# no clusters, stratified, with finite populaton correction
dstrat = survey::svydesign(id=~1, strata=~stype, weights=~pw, fpc=~fpc)

dclus = svydesign(id=~clus, weights=~pw)       # clustered design

svymean(~y, dclus, deff=TRUE)                  # survey weighted mean
svytotal(~z, dclus, deff=TRUE)                 # survey weighted total
svyratio(numerator = ~y, denominator= ~z, dclus, deff=TRUE)    # survey weighted ratio
svyglm(y ~ x, data=dstrat, family='binomial')  # standard glm
```

There are also some other models like survival models, factor analysis, and more.  One thing I can also mention- don't use the `weights` argument in the base R <span class="func">lm/glm</span> functions with survey weights.  That argument refers to a different type of weight (inverse variance).


## Survey analysis list of packages

```{r survey_percs, eval=TRUE, echo=FALSE}
survey_perc = get_perc('survey')
```

- <span class="pack">survey</span> `r survey_perc`%

## Survey analysis recommended reading

@lumley2011complex

# Principal components & Factor analysis

A lot of techniques fall under the heading of 'dimension reduction', 'factor analysis', or 'matrix factorization' techniques.  


## PCA

Principal components analysis is one of the most widely used dimension reduction techniques, and brings us to the first example of unsupervised methods.  In this case, we don't have a single target variable to predict, but several, and generally we are not modeling them with other covariates, but trying to reduce them.  The basic idea of PCA is to reduce the data to fewer dimensions that nevertheless retain the most variance seen in the data.

Base R has <span class="func">princomp</span> and <span class="func">prcomp</span> functions, but they merely get the job done.  There are plenty of extensions of PCA to supervised settings, but you can do that exploration on your own.  

```{r pca_demo}
X = scale(X)          # standardize the data
princomp(X)           # via eigen decomposition
prcomp(X)             # via singular value decomposition
```

Beyond that you'll have some methods like <span class="func">loadings</span>, and these will often work in other packages as well.  The most common things you'll want are scores and loadings.

```{r pca_methods}
X$scores(X)           # component scores
X$loadings(X)         # loadings/weights
loadings(X)
```

Note that this recommendation comes from someone who has little use for PCA (but very often does factor analysis), I can recommend a couple other packages.  The <span class="pack">psych</span> package puts PCA more squarely in the framework of factor analysis more generally, and offers rotations, other measures of fit, etc.  The <span class="pack">pcaMethods</span> package provides probabilistic PCA, Bayesian PCA, cross-validation,  selection of the number of components, PCA-based imputation, and more.

```{r pca_extend}
model = psych::principal(X, nfactors = 2, rotate='varimax')
model$loadings
model$scores

model = pcaMethods::ppca(X, nPcs = 2)      # probabilistic PCA
loadings(model)
scores(model)
```

There are plenty of supervised extensions to PCA, but generally you'll have better choices.  For example, instead of PC regression, use techniques that won't require the dimension reduction, would better to such a process without being as restrictive (like a neural network), or would better suit the goals of some analyses (e.g. when variables belong to the same scale).

## Factor analysis

The base R approach to the most common form of factor analysis is pretty much identical to PCA.  

```{r fa_demo}
model = factanal(X, factors = 2, scores = 'regression', rotation = 'promax')
model$loadings
model$scores
```


The <span class="pack">psych</span> package offers far, far, more in this realm.  The output alone will take a bit for you to get through.

```{r fa_psych}
model = psych::fa(X, nfactors = 2, rotate = 'oblimin', fm = 'ml')  # via maximum likelihood
model$loadings
model$scores
fa.diagram(model)                     # visualize
predict(model, newdata=mynewdata)     # estimate scores for new data
nfactors(X, 4, rotate='promax')       # compare solutions for 1:4 factors
```

Once you get into latent variable models like factor analysis, you'll have plenty of tools at your disposal.  Check out <span class="pack">lavaan</span> (confirmatory factor analysis/SEM) and <span class="pack">ltm</span> (item response theory models) for starters.

## PCA/FA list of packages

[Psychometrics Task View](https://www.rdocumentation.org/taskviews#Psychometrics)

```{r pcafa_percs, eval=TRUE, echo=FALSE}
psych_perc = get_perc('psych')
pcaMethods_perc = get_perc('pcaMethods')
lavaan_perc = get_perc('lavaan')
ltm_perc = get_perc('ltm')
```

- <span class="pack">psych</span> `r psych_perc`%
- <span class="pack">pcaMethods</span> (not accurately tracked since on BioConductor)
- <span class="pack">lavaan</span> `r lavaan_perc`%
- <span class="pack">ltm</span> `r ltm_perc`%


## PCA/FA recommended reading

I have some more on many dimension reduction techniques [here](https://m-clark.github.io/sem/FA_notes.html), which briefly covers/demos a wide variety of techniques like t-sne, non-negative matrix factorization, latent Dirichlet allocation, mixture models, recommender systems and more.  It spends more time on PCA and FA.

I also cover PCA/FA [here](https://m-clark.github.io/sem/), along with SEM, mixture models, IRT models and more.

For measurement in general, consider psychometrician and <span class="pack">psych</span> package author [William Revelle's freely available text](http://www.personality-project.org/r/book/).  You will also learn a ridiculous amount by simply reading the help files to the various functions in his package.

More to follow in the [SEM][Structural Equation Modeling] section.



# Mixture models and cluster analysis

Often the goal is to find latent structure (or simply dimension reduction) among the observations (rows), as opposed to the variables (columns).  As such we have another unsupervised modeling situation that is very commonly used- cluster analysis.  

## Mixture models

We'll start with model-based approaches to the problem, typically referred to as <span class="emph">mixture models</span>. A good starting point here is <span class="pack">mclust</span>.  It provides a very easy way to search across a wide variety of cluster types, as well as any number of clusters (including 1 cluster).  You may then select the 'best' solution based on BIC, with is the advantage of a model-based approach.  Furthermore, it provides plots of the search space, the classifications, uncertainty and density plots.

```{r cluster_mclus}
model = mclust::Mclust(X, G = 1:4)
summary(model)
plot(model)
```

Often people want to do a cluster analysis and then use those clusters in a subsequent analysis, e.g. as a predictor.  This two-stage approach fails to take into account the uncertainty in the class assignment.  However, the mixture model approach can do this appropriately, and we have another long-standing R package to help us there- <span class="pack">flexmix</span>.  This package can actually handle a wide variety of regression models (e.g. different response distributions, mixed models[^lgc]), allowing the results to vary across the latent clusters.  It uses S4 class objects.

```{r cluster_flexmix}
model = flexmix::flexmix(yn ~ x, k = 2)  # two clusters
summary(model)                    # basic info
parameters(model)                 # coefficients (for each cluster/class)
posterior(model)                  # probability of class membership

# two outcomes, two models, two classes
model = flexmix(yn ~ x, k = 2,
                model = list(FLXMRglm(yn ~ x + z), 
                             FLXMRglm(yp ~ x + offset(q), family = 'poisson')))
```


Another type of model, or at least term, you may here with regard to mixture models is <span class="emph">latent class analysis</span>.  The only distinction here is that the data is categorical.  The <span class="pack">poLCA</span> package focuses on this.  It also will allow for modeling the latent classes as an outcome (in distinction to <span class="pack">flexmix</span>), just as you would in standard logistic regression settings.

```{r cluster_polca}
# assume X is a matrix of binary indicator variables
model = poLCA::poLCA(X ~ 1, values, nclass = 2)      # basic cluster analysis
model = poLCA(X ~ x + z, values, nclass = 2)         # add predictors
poLCA.posterior(model)                               # probability of class membership
```

While it does about as much as many would want, unfortunately <span class="pack">poLCA</span> is pretty bare bones. Look for <span class="pack">lavaan</span> to add latent classes in the future.

When it comes to modeling with latent clusters, there's a lot of ways to do so.  A very common approach that one sees in other modeling frameworks is hidden Markov models.  Consider the <span class="pack">hmm</span> package as a starting point, but there's a lot out there.

## Traditional cluster analysis

For traditional cluster analysis based on distance matrices, one doesn't really need to go beyond the base R installation, as it comes with <span class="func">dist</span>, <span class="func">kmeans</span>, <span class="pack">hclust</span> functions, and the <span class="func">recommended</span> package cluster.

```{r cluster_traditional}
X_dist = dist(X)                          # euclidean distances
X_dist = dist(X, method = 'manhattan')    # manhattan distance

kmeans(X_dist, 3)                         # k-means 3 cluster solution

hclust(X_dist, method = 'average')        # hierarchical clustering with average linkage
cluster::agnes(X_dist)                    # agglomerative clustering
cluster::diana(X_dist)                    # divisive clustering
```

Unfortunately such methods come with many arbitrary choices- distance metric, linkage method, number of clusters, clustering method, 'performance', etc.  I'd suggest something like <span class="pack">clusterSim</span> to make it easy to search through the minimally dozens of options you might consider.  For example, the following would search over several clustering approaches, evaluating them across different cluster numbers, cluster quality assessments, distance metrics, normalization techniques and so forth.

```{r cluster_sim}
clusterSim::cluster.Sim(X, 
                        p = 1,
                        minClusterNo = 2,
                        maxClusterNo = 5,
                        icq = c('G1', 'S'),
                        distances = c('d2', 'd5'),
                        normalizations = c('n1', 'n3'),
                        methods = c('m5', 'm3', 'm1'))
```


## Cluster analysis list of packages

```{r cluster_percs, eval=TRUE, echo=FALSE}
Mclust_perc = get_perc('Mclust')
flexmix_perc = get_perc('flexmix')
hmm_perc = get_perc('hmm')
polca_perc = get_perc('polca')
clusterSim_perc = get_perc('clusterSim')
cluster_perc = get_perc('cluster')
```


- <span class="pack">mclust</span> `r Mclust_perc`%
- <span class="pack">flexmix</span> `r flexmix_perc`%
- <span class="pack">hmm</span> `r hmm_perc`%
- <span class="pack">polca</span> `r polca_perc`%
- <span class="pack">clusterSim</span> `r clusterSim_perc`%
- <span class="basepack">cluster</span> `r cluster_perc`%

## Cluster analysis recommended reading

[Cluster Analysis Task View](https://www.rdocumentation.org/taskviews#Cluster)

@everitt2011cluster Provides an overview of traditional methods in a clean manner.  Can't speak much to more modern references.

I have a bit on mixture models and latent class analysis from the SEM perspective [here](https://m-clark.github.io/sem/mixture-models).




# Structural Equation Modeling

<span class="emph">Structural Equation Modeling</span> (SEM) is a very broad technique that encompasses and extends many others, bringing together regression, factor analysis, mixtures models and more. In particular it combines measurement models regarding latent variables and observed variables, and structural models to explore the effects of latent and observed variables on others.

Due to the use of latent variables and other specifics, a special syntax is required to run SEM models, but otherwise it does not require much to get going, at least with the <span class="pack">lavaan</span> package.  For those familiar with Mplus, it and various extensions can do about 90% of what Mplus does[^lavlack], and even spits out results in the same format.

## Path Analysis

We can start with a <span class="emph">path analysis</span> using only observed variables.  If you know how to run a regression, you can do this easily.

```{r path_lavaan}
model = "
  y ~ x + q + r + z
  z ~ x + q
"

result = lavaan::sem(model)
summary(result, standardized=TRUE)  # result with standardized effects
```

## Confirmatory Factor Analysis

<span class="emph">Confirmatory factor analysis</span> is just like any other factor analysis, except some loadings are constrained to be zero, usually for theoretical and other not so good reasons.

```{r cfa_lavaan}
model = "
  lv1 =~ x1 + x2 + x3
  lv2 =~ y1 + y2 + y3

  y1 ~~ x1     # residual correlation to be estimated
"

result = cfa(model)
lavPredict(result)   # get factor scores
```

## SEM

SEM allows us to measure multiple outcomes, indirect effects, and combine latent and observed variables, all at once.

```{r sem_lavaan}
model = "
  lv1 =~ x1 + x2 + x3
  lv2 =~ y1 + y2
  lv3 =~ z1 + z2 + z3
  
  # a mediation model with lv2 as mediator
  lv3 ~ lv1 + lv2 + q + f    # q and f are observed
  lv2 ~ lv1 + q
"

# sem with an approach to deal with missing values, robust ML, additional fit measures
sem(model, missing='fiml', estimator='MLR', fit.measures=TRUE)  
```

### lavaan extensions

Several packages extend the capabilities of <span class="pack">lavaan</span>. 

```{r lavaan_getgraph, eval=TRUE, echo=FALSE}
lavaan_depgraph = get_depgraph('lavaan')
```

```{r lavaan_graph, echo=F, eval=T}
nodes = lavaan_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = lavaan_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

edge_trim = edges %>% 
  filter(to == 0)
visNetwork(nodes = nodes %>% 
             filter(id %in% c(0, edge_trim$from)) %>% 
             mutate(value=c(100, rep(0, nrow(.)-1))), 
           edges=edge_trim) %>% 
  visEdges(color=list(opacity=.10))
```

In terms of Mplus style extensions, note <span class="pack">lavaan.survey</span> (survey design), <span class="pack">blavaan</span> (Bayesian), and <span class="pack">semTools</span>, which adds things like multiple imputation, measurement invariance testing, additional fit indices and a lot more.

A couple others I will note but with which I haven't used yet.  I note them because I will at some point, and their intentions are in the right place. One is <span class="pack">regsem</span>.  The vast majority of SEM models are grossly overfit, estimating dozens of parameters with only a couple hundred observations.  Such a package can surely assist with this problem, though I predict many will not like what they find (even though it will be a better result).  In addition, while writing this I came across the <span class="pack">lavaanPlot</span> package, which might save me at least 50% of the headache of using <span class="pack">Diagrammer</span>.


### Beyond lavaan

Going beyond lavaan, I would suggest looking into the <span class="pack">mediation</span> package if you only have observed variables, as it can handle a wide variety of models for the mediator and outcome.  In a similar vein check out <span class="pack">piecewiseSEM</span>. <span class="pack">MplusAutomation</span> will save you from having to use Mplus even if you want to use it.  With one template you can write the syntax files, run the models, and bring back the results into R in a usable form, and all automatically, without ever opening Mplus.

Others that might be of note but I can't speak to include <span class="pack">lava</span> and <span class="pack">OpenMx</span>.  In addition, the aforementioned <span class="pack">psych</span> package has some SEM/lavaan capabilities.


## SEM analysis list of packages

```{r sem_percs, eval=TRUE, echo=FALSE}
lavaan_perc = get_perc('lavaan')
lavaan_survey_perc = get_perc('lavaan.survey')
blavaan_perc = get_perc('blavaan')
lavaanPlot_perc = get_perc('lavaanPlot')
semTools_perc = get_perc('semTools')
regsem_perc = get_perc('regsem')
mediation_perc = get_perc('mediation')
piecewiseSEM_perc = get_perc('piecewiseSEM')
MplusAutomation_perc = get_perc('MplusAutomation')
lava_perc = get_perc('lava')
OpenMx_perc = get_perc('OpenMx')
psych_perc = get_perc('psych')
```


- <span class="pack">lavaan</span> `r lavaan_perc`%
- <span class="pack">lavaan.survey</span> `r lavaan_survey_perc`%
- <span class="pack">blavaan</span> `r blavaan_perc`%
- <span class="pack">lavaanPlot</span> `r lavaanPlot_perc`%
- <span class="pack">semTools</span> `r semTools_perc`%
- <span class="pack">regsem</span> `r regsem_perc`%
- <span class="pack">mediation</span> `r mediation_perc`%
- <span class="pack">piecewiseSEM</span> `r piecewiseSEM_perc`%
- <span class="pack">MplusAutomation</span> `r MplusAutomation_perc`%
- <span class="pack">lava</span> `r lava_perc`%
- <span class="pack">OpenMx</span> `r OpenMx_perc`%
- <span class="pack">psych</span> `r psych_perc`%


## SEM recommended reading

[Psychometrics Task View](https://www.rdocumentation.org/taskviews#Psychometrics)

My doc on SEM covers a lot of ground and provides more extensive examples.

@kline2015principles is a very popular applied treatment. See the classic @bollen1989 for more details regarding standard models.



# Graphical models

SEM can be seen as a very large subset of what are generally called <span class="emph">graphical models</span>, where variables are represented by <span class="emph">nodes</span> (sometimes called vertices) and the relations among them are represented by <span class="emph"></span> (sometimes called arcs).  Edges may be <span class="emph">directed</span> (i.e. have arrows) or <span class="emph">undirected</span>.  With directed (acyclic) graphs (DAG), we get into things like causal modeling, SEM, bayesian networks, etc. With undirected graphs, often called a Markov random fields, applications are in network models, spatial analysis, and more.

In most cases there are two main approaches to running such models.  One is where two data sets are provided, one which specifies the nodes and various attributes associated with them. In addition, one has an edge list or data frame that specifies the nodes from which a connection begins, the nodes to which they connect, and possibly a column specifying the value/weight for the connection.  The other approach regards an <span class="emph">adjacency matrix</span>, which is like a correlation or (inverse) distance matrix, where the off-diagonal values specify connections among the nodes, and may be simply binary or comprise continuous <span class="emph">weights</span>.

One of the more popular and powerful packages is igraph, and even when you're using others, they are often using it under the hood. Given the right data, one can create a graph object, and then get any number of statistics, visualize it, or whatever.  A lot of it is for the creation of graphs, but normally graphs are too big to do by hand and/or you'd have the data already, and it'd be easier to create an adjacency matrix from your typical data frame and work from there.

```{r gm_igraph, echo=-1}
# g = igraph::make_graph("Zachary")  # a social network
g = from_adjacency(adjmat)     # create graph from adjacency matrix
g = from_data_frame(df)        # create graph from a data frame
as_adjacency_matrix(g)         # convert graph to adjacency
plot(g)                        # plot it
degree(g)                      # degree for each node
hub_score(g)                   # hub score for each node
count_triangles(g)             # N triangles for each node
clusters = cluster_edge_betweenness(g)  # detect clusters
membership(clusters)
communities(clusters)
```

The basic plot functionality for igraph is... basic.  It's not worth your time to make it pretty.  To that end I suggest packages like networkd3, visNetwork, and Diagrammer.  It's not too big a deal to convert the graph object to something they will use (visNetwork will even work with an igraph object, e.g. with toVisNetworkData).

### Bayesian networks

A package that's useful for bayesian networks, or probabilistic DAG, is bnlearn.  In this case we are looking for a graphical structure based on the data we have. The package allows for model fitting such that you could essentially duplicate a path analysis SEM-style, but the strength is in the model search and selection via a variety algorithms, possibly with cross-validation.  It can work with binary, continuous, or a mix of variables.  With blacklists and whitelists, one can impose theoretical structure on the search such that some connections are always present while others are forbidden.

```{r gm_bnlearn}
model = gs(X, blacklist = bl)   # grow-shrink algorithm with a blacklist
plot(model)
```

### Network analysis


ggm
bnlearn
network sna 

igraph
networkd3
visnetwork
Diagrammer

## list of packages

## recommended reading

[Graphical Model Task View](https://www.rdocumentation.org/taskviews#gR)


# Time series

[Time Series Task View](https://www.rdocumentation.org/taskviews#TimeSeries)


xts, forecast, prophet, lubridate, tseries, arima


```{r ts-funcs}
acf
lag
posix
as.Date()
```


## list of packages
## recommended reading

# Spatial models
[Spatial Task View](https://www.rdocumentation.org/taskviews#Spatial)
[SpatialTemporal Task View](https://www.rdocumentation.org/taskviews#SpatioTemporal)

spatial
sf
sp
spdep
mgcv
ggmap
leaflet
plotly

## list of packages
## recommended reading

# Machine learning

You can find detailed examples in my document [here](https://m-clark.github.io/introduction-to-machine-learning).

[Machine Learning Task View](https://www.rdocumentation.org/taskviews#MachineLearning)

e1071
caret
mlr
glmnet
randomForest
gbm
xgboost
lime

## list of packages
## recommended reading

# Bayesian

[Bayesian Task View](https://www.rdocumentation.org/taskviews#Bayesian)

rjags
r2OpenBugs
coda
MCMCpack
MCMCglmm
R-INLA

## Stan

rstan
rstanarm
brms

## Other
greta
tidybayes


## list of packages
## recommended reading

# Text

You can find detailed examples in my document [here](https://m-clark.github.io/text-analysis-with-R).

[Task View](https://www.rdocumentation.org/taskviews#NaturalLanguageProcessing)


tidytext
text2vec
stringdist

tm
lda
topicmodels
keras

## list of packages
## recommended reading

# Censored, truncated, and related

glm.nb, glmer.nb
survival
pscl
VGAM, gamlss



# Discipline specific

Probably should avoid

## Ecological
[Task View](https://www.rdocumentation.org/taskviews#Environmetrics)
vegan
Zuur et al.


## Financial

# Modeling frameworks and helpers

Probably not enough to (consistently) do much here.

caret zelig recipes tm? sjstats car


# References

[^jagam]: There is the <span class="func">jagam</span> function in <span class="pack">mgcv</span> to use JAGS, but I can't think of a reason to use it over <span class="pack">brms</span>.

[^lgc]: Using a mixed model on longitudinal data with <span class="pack">flexmix</span> negates the need to use SEM for what are sometimes called <span class="emph">Latent Class Growth Curve Models</span>, <span class="emph">Growth Mixture Models</span> or <span class="emph">Latent Trajectory Models</span>.  If you need indirect effects, you'll have to use Mplus or similar, and good luck with that.

[^lavlack]: Currently lavaan lacks the incorporation of categorical latent variables (i.e. mixture models), but you have more functionality in R for those elsewhere than you would Mplus. The current lavaan also lacks multilevel SEM, but that is a pretty rare model.  There are very minor other differences.