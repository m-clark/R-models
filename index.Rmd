---
title: <span style="font-size:125%; font-family:'Questrial'; font-style:normal"><img src='img/198R.png' style="display:inline; vertical-align:bottom; height:1.25em; width:1.25em" ></img> Models</span>
subtitle: <span style="font-size:125%; font-style:normal; font-variant:small-caps; font-family:'Questrial'">Quick Reference</span>
author:  |
  <span style="font-size:125%; font-style:normal; font-family:'Questrial'">Michael Clark</span> <br>
  <span class="" style="font-size:75%">https://m-clark.github.io</span><br><br>
output:
  html_document:
    css: [css/standard_html.css]
    number_sections: false
    df_print: kable
    fig_caption: yes
    highlight: pygments
    # theme: sandstone
    toc: true
    toc_depth: 2
    toc_float:
      collapse: section
      smooth_scroll: false    
      scroll_highlight: yes
bibliography: refs.bib
biblio-style: apalike
link-citations: yes
output_dir: "docs"
description: ""    
font-import: https://fonts.googleapis.com/css?family=Roboto|Roboto+Mono|Open+Sans|Alex+Brush|Questrial|Stalemate
cover-image: img/nineteeneightyR.png
url: 'https\://m-clark.github.io/' 
favicon: 'img/R.ico'
github-repo:  m-clark/
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = T, eval=F, message=F, warning=F, error=F, comment=NA, R.options=list(width=220),   # code 
                      dev.args=list(bg = 'transparent'), dev='svglite',                                 # viz
                      fig.align='center', out.width='75%', fig.asp=.75,                 
                      cache.rebuild=F, cache=T)                                                         # cache
```

```{r packages, include=FALSE, cache=FALSE, eval=TRUE}
library(magrittr); library(tidyverse); library(stringr); library(pander); 
library(plotly); library(lazerhawk); library(viridis); library(visNetwork)
```



```{r rdoc_api, echo=FALSE, eval=TRUE}
# Function to get package percentile rank
get_perc = function(pack) {
  require(httr)
  url = paste0('http://rdocumentation.org/api/packages/', pack, '/percentile')
  out = GET(url) %>% 
    content()
  out$percentile
}
get_depgraph = function(pack) {
  require(httr)
  url = paste0('http://rdocumentation.org/api/packages/', pack, '/reversedependencies')
  out = GET(url) %>% 
    content()
  node_df = data.table::rbindlist(out$nodes, fill = T)
  edge_df = data.table::rbindlist(out$links, fill = T)
  list(node_df=node_df, edge_df=edge_df)
}
```

THIS DOCUMENT IS A WORK IN PROGRESS. IT IS CURRENTLY NOT COMPLETE.

# Introduction

This is a quick reference for the modeling syntax and associated packages.  While it covers a lot of ground, it is not meant to be exhaustive, but rather, it provides an easy reference for those new to R, someone trying out an unfamiliar (but otherwise common) technique, or those just interested in comparison to similar approaches in other environments.  In time, this may be rendered obsolete by the [parsnip package](https://github.com/topepo/parsnip), but until then, this can get you quickly started with many common models.

Preference is given to base R functionality or what are perceived as very commonly used packages.  'Very common' is based on my own knowledge consulting across dozens of disciplines along with just regularly following the R community, and things like the rankings at [RDocumentation.org](https://rdocumentation.org), and packages listed in [CRAN Task Views](https://www.rdocumentation.org/taskviews).



#### Miscellaneous

In what follows, you will not see any model which I have not run on my own, if not many, many times. For those who want to get more into the implementation of many of these models, I have raw R code that demonstrates many (probably most) of the techniques discussed [here](https://github.com/m-clark/Miscellaneous-R-Code).

I will ignore the data argument in every example that uses the typical R formula approach. It is implied that you would supply it. 

What this won't provide:

- data processing required, except where it is specifically tied to the analysis
- a detailed explanation of the model and why you'd use it
- visualization (though some packages will be mentioned for specifically visual approaches)
- a list of every package that *might* be useful

For some content I will provide recommended reading.  These are texts are mostly those I've benefited from or I would think would be useful to others, especially those just starting out with analysis, at an intermediate stage, or even coming to R from another language.  They are not meant to be a list seminal works on the subject, though some might be.  I also include the Task View if applicable. The link will take you to Rdocumentation.org, which will list many packages, but which the first link is the official Task View, where you will find more details about the packages.


Color coding:

- <span class="emph">emphasis</span>
- [link]()
- <span class="pack">package</span>
- <span class="basepack">package that comes with standard R installation</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>

When you see `pack::func` the first value is the package name and the second the function.  I show this for the first time to make clear what package is used, but otherwise it is assumed you've loaded the necessary package (`library(pack)`).


# Basics

Most packages follow the same basic modeling syntax, and those who don't generally suffer less usage.  Just specify your target variable, say `y` and your predictor variables, e.g. `x`.  For some packages you may have to instead supply a model.matrix `X` and separate `y` vector (with no data argument).


## Standard modeling

```{r basics-standard-modeling}
model_func(y ~ x, data=mydata)
model_func(X, y)
model_func(X)  # unsupervised (e.g. principal components analysis)
```

Some models may be different enough to warrant their own syntax, e.g. latent variable models/SEM.  However, even then, packages try to adhere to the `y ~ x` formula approach.

## Exploring Results

Most modeling functions, again, those that actually want to be used, will provide the following commonly used methods.

```{r basics-explore}
summary(my_model)          # may also be just print()
fitted(my_model)           # fitted values
residuals(my_model)        # residuals
coef(my_model)             # extract coefficients
model.matrix(my_model)     # extract the model matrix
predict(my_model, newdata=other_observations, type='response')  # specific predictions
```

Some may also have a <span class="func">plot</span> method, but this will definitely vary by package both in whether it is available and what it will do.  For example, plotting an <span class="objclass">lm</span> object will explore residuals, while plotting a <span class="objclass">gam</span> will show the component plots of the smoothed terms. Some packages, like <span class="pack">ggeffects</span>, <span class="pack">visreg</span> and <span class="pack">margins</span>, will help plotting some types of effects.  I've found it easier to do oneself via <span class="pack">ggplot2</span>, rather than hope you can finagle a plot made by a package into something you actually want.

One way to prepare results for visualization and publishable tables is the <span class="pack">broom</span> package.  Its tidy function works with many very commonly used modeling packages, returning a <span class="func">tidy</span> data frame of, for example, the summary output.

```{r basics-broom}
broom::tidy(my_model)
```


Many packages will also have a <span class="func">confint</span> method for interval estimates of coefficients.  See the <span class="basepack">boot</span> package as a starting point for bootstrapped intervals.

## Comparing models

Good science will need competing models.  For many models, one can use a likelihood ratio test or something like AIC.

```{r model_comparison}
anova(mod)
anova(mod1, mod2)

AIC(mod)
AIC(mod1, mod2)

lrtest(mod1, mod2)    # only for some packages
```

Others will be package specific. For example, <span class="basepack">mgcv</span> will supply a GCV (generalized cross-validation) metric, and there is WAIC and loo in <span class="emph">Stan</span>-related packages for Bayesian analysis.

## Optimization

If you want to roll your own model, the base R installation comes with <span class="func">optim</span>, but you're probably better suited to use one of the many packages that provide more efficient[^optim] or simply better implementations of the same algorithms.  See the [Task View](https://www.rdocumentation.org/taskviews#Optimization) for starters, and know that many will work in a similar fashion as <span class="func">optim</span>.

```{r optim_demo}
model = optim(start_values, my_opt_function, method = 'BFGS', control = list(maxit=1000))
model

meths <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "nlm", "nlminb")

result <- optimr::opm(start_values, my_opt_function, method=meths)
summary(result, order=value)
```


# Linear models and GLM 

Starting with the basics, we can obtain linear models (OLS) or generalized linear models as follows.

```{r glm}
aov(y ~ x + Error(id))                    # ANOVA with error stratum (e.g. repeated measures for each id)
lm(y ~ x + z)                             # standard linear model/OLS
glm(y ~ x + z, family = 'binomial')       # logistic regression with binary response
glm(y ~ x + z + offset(log(q)), family = 'poisson')  # count/rate model
```

See also, <span class="pack">rms</span>::<span class="func">ols</span> for the standard model with perks, along with <span class="func">lrm</span> for logistic regression.


## Extensions

### Interactions and Variable transformations

Unlike some statistical packages that haven't figured out what year it is, you do not have to make explicit interaction terms.  There are two ways you can do it.

```{r glm-interactions}
lm(y ~ x*z)
lm(y ~ x + z + x:z)  # equivalent
```

There are better approaches to get at nonlinear relationships than with polynomials (variables interacting with themselves), but if interested, one can use <span class="func">poly</span>.  There is also <span class="func">nls</span> for nonlinear least squares.  However, it is rare that one knows the functional form beforehand.  Just because you can fit a quadratic function or logistic growth curve model doesn't mean it's actually the right or best functional form.

```{r glm-polynomial}
lm(y ~ poly(x, degree=3))
nls(y ~ SSlogis(log(x), Asym, xmid, scal))  # see ?SSlogis for details
```

You typically can transform a variable within the formula itself.

```{r glm-transformations}
lm(y ~ log(x))
```

This is fine for quick exploration, but generally it's poor practice to do so (you'll likely need the transformed variable in other contexts), and it simply won't work with some modeling functions.

### Categorical variable

#### Predictors

For categorical predictors, most modeling packages will automatically dummy code any categorical variables.  But you can specify other coding schemes as well.

```{r glm-coding}
lm(y ~ x)  # x is a factor
?contrasts
```


#### Ordinal target

Options thin out quickly when you have more than 2 categories for your target variable.  But there is still plenty to play with in R for ordinal and multinomial models.

For ordinal models, the <span class="pack">rms</span> package is a very good starting point. The <span class="pack">ordinal</span> package will add random effects as well, in the familiar <span class="pack">lme4</span> style.

```{r glm-ordinal}
rms::orm(y ~ x)  # y is ordinal
ordinal::clmm(y ~ x + (1|g1) + (1|g2), link = "probit", threshold = "equidistant")
```


#### Multinomial

For nominal dependent variables, check out the <span class="pack">mlogit</span> package.  You will almost certainly have some data processing to do beforehand.

```{r glm-multinomial}
# x: is an generic covariate
# q: is an individual specific covariate
# z: is an alternative specific covariate
mlogit::mlogit(y ~ x|q|z)

?mlogit::mlogit.data
?mlogit::mFormula
```

See <span class="pack">mnlogit</span> for a faster approach. Technically the base R package <span class="basepack">nnet</span> will do these models too, but you won't be able to do much with the result.  

### Other distributions

The standard GLM will only take you so far in terms of distributions for the target variable.  To start your journey beyond the exponential family, you might consider the following.  Some packages will provide additional distributions in similar contexts, and some might be geared entirely toward a specific family.

- <span class="pack">glm.nb</span>: Base R function for the negative binomial
- <span class="pack">pscl</span>: Zero-inflated and hurdle models
- <span class="pack">betareg</span>, <span class="pack">dirichletReg</span>: Beta (0, 1) and Dirichlet distributions (e.g. compositional data)
- <span class="pack">VGAM</span>, <span class="pack">mgcv</span>, <span class="pack">gamlss</span>: Additive modeling packages that come with many additional distributions (e.g. student T, bivariate logit/probit, censored normal, tweedie, multivariate gaussian, etc.)

#### Censored, truncated, bounded, and related

Sometimes we have a response that doesn't quite fit the mold of commonly used distributions. A common situation regards a target variable of counts, where there are an inordinately large amount of zero counts where the event of interest did not occur.  In other cases, the scale is truncated at some value, or we may simply not see scores beyond a certain point.  In general we can consider <span class="emph">zero-altered</span> models, <span class="emph">truncated</span> distributions, and <span class="emph">censored</span> regression models.

Probably the most common of these situations is the zero-altered case.  A good starting point for such models is the <span class="pack">pscl</span> package. For <span class="emph">zero-inflated</span> or <span class="emph">hurdle</span> models, you'll get two sets of output, a model that regards the zeros and one that regards the target variable in general. For censored regression, I provide an example with <span class="pack">AER</span>, as there is a common contributor to that package. Note that it is just using <span class="func">survreg</span> from the <span class="pack">survival</span> package.

```{r other_zero}
model = pscl::hurdle(y ~ x + z, dist = "negbin")  # hurdle model with negative binomial response
model = zeroinf(y ~ x + z, dist = "posi")         # zero-inflated model with negative binomial response
summary(model)

model = AER::tobit(y ~ x + z, left = 0)           # left-censored at 0 
```

For truncated distributions you likely won't have a lot of options unless you're willing to go into the Bayesian realm, but you can start with <span class="pack">VGAM</span> as a non-Bayesian option.

```{r other_trunc}
model = VGAM::vglm(y ~  x + z, truncpareto(lower, upper))  # truncated Pareto
```

Another common situation is where the data is bounded by [0,1], but isn't based on a binomial distribution, or one doesn't have access to the underlying counts if it is.  The beta distribution is a good option in this case, and the <span class="pack">betareg</span> package provides some modeling just for that. 

```{r other_beta}
model = betareg::betareg(y ~  x + z)
```

Note that many modeling packages might offer the standard beta as a distributional family though.


### Miscellaneous

Some might be interested in quantile regression.  The <span class="pack">quantreg</span> package would be a starting point.  In addition, the <span class="pack">car</span> package has a few miscellaneous functions for various models, e.g. add-variable plots, variance inflation factors, influence metrics, and more.

## GLM list of packages

```{r lm_percs, eval=TRUE, echo=FALSE}
# roughly in order
rms_perc = get_perc('rms')
ordinal_perc = get_perc('ordinal')
mlogit_perc = get_perc('mlogit')
mnlogit_perc = get_perc('mnlogit')
nnet_perc = get_perc('nnet')
mgcv_perc = get_perc('mgcv')
VGAM_perc = get_perc('VGAM')
gamlss_perc = get_perc('gamlss')
pscl_perc = get_perc('pscl')
betareg_perc = get_perc('betareg')
dirichletReg_perc = get_perc('dirichletReg')
quantreg_perc = get_perc('quantreg')
car_perc = get_perc('car')
```


The following is a list of the packages mentioned in this section and their percentile rank on RDocumentation.

- <span class="pack">rms</span> `r rms_perc`%
- <span class="pack">ordinal</span> `r ordinal_perc`%
- <span class="pack">mlogit</span> `r mlogit_perc`%
- <span class="pack">mnlogit</span> `r mnlogit_perc`%
- <span class="basepack">nnet</span> `r nnet_perc`%
- <span class="basepack">mgcv</span> `r mgcv_perc`%
- <span class="pack">VGAM</span> `r VGAM_perc`%
- <span class="pack">gamlss</span> `r gamlss_perc`%
- <span class="pack">pscl</span> `r pscl_perc`%
- <span class="pack">betareg</span> `r betareg_perc`%
- <span class="pack">dirichletReg</span> `r dirichletReg_perc`%
- <span class="pack">quantreg</span> `r quantreg_perc`%
- <span class="pack">car</span> `r car_perc`%

## GLM recommended reading


@harrell2015

@gelman_arm

@hardin_generalized_2012



# Regularization

Not enough people use <span class="emph">regularization</span> in their models to guard against overfitting, and it should be the default approach in my opinion, especially in small data/complex modeling scenarios.  The <span class="pack">glmnet</span> package fits a variety of models, with <span class="emph">lasso</span>, <span class="emph">ridge</span>, or something in between for a penalty.  Not a very user friendly package though.

```{r reg-glmnet}
glmnet::glmnet(X, y)
```


See also, <span class="pack">elasticnet</span>.  Better yet, go [Bayesian][Bayesian].  Note also that there are actually quite a few packages that will accept a penalty parameter/matrix, but odds are slim you'd know that in advance. 

## Regularization list of packages

```{r reg_percs, eval=TRUE, echo=FALSE}
glmnet_perc = get_perc('glmnet')
elasticnet_perc = get_perc('elasticnet')
```


- <span class="pack">glmnet</span> `r glmnet_perc`%
- <span class="pack">elasticnet</span> `r elasticnet_perc`%

# Mixed models

One of the most common generalizations from the GLM setting regards data dependent situations where observations are correlated with one another due to inherent clustering.  This is most commonly dealt with via <span class="emph">mixed models</span>.  You can find detailed examples in my document [here](https://m-clark.github.io/mixed-models-with-R).

## nlme

The <span class="pack">nlme</span> package  comes with base R.  It does standard linear and nonlinear (in the predictors) mixed models. It can add correlation structure (e.g. temporal (e.g. <span class="func">corAR</span>) and spatial (e.g. <span class="func">corGaus</span>)), as well as heterogeneous variances (e.g. <span class="func">varIdent</span>). It can also do nonlinear models of specific functional forms (e.g. <span class="func">SSlogis</span>).  However, it cannot generalize beyond the gaussian distribution.

```{r nlme}
nlme::lme(y ~ x, random = ~ 1|group)       # basic model
lme(y ~ x, random = ~ 1 + x|group)         # add random slopes
lme(y ~ x, random = list(~ 1|g1, ~ 1|g2))  # different random effects
```


In addition, one can get at additional correlation/variance structure, though it may not be very intuitive.  The first line examines heterogeneous variances across some grouping factor `q`.  The other demonstrates an autoregressive correlation structure.

```{r nlme_cor}
lme(y ~ x, random = ~ 1|group, weights = varIdent(form=~1|q))
lme(y ~ x, random = ~ 1|group, correlation = corAR1(form=~1|q))
```


## lme4

The lme4 package is one of the most widely used modeling packages in R.  It's the best tool for mixed models, and I've used many within R and beyond.  It doesn't do everything, but it does a lot, and fast.  It serves most people's needs just fine.  Beyond that, many other packages work or depend on it, and other packages that extend lme4 will use the same modeling syntax.

In contrast to nlme, <span class="pack">lme4</span> is very efficient but does not have the capability (in a practical way) to do things like further examination of residual structure.  It does however extend the models to the GLM family (and negative binomial with glmer.nb).

```{r lme4}
lme4::lmer(y ~ x + (1|group))
lmer(y ~ x + (1|g1) + (1|g2))
lmer(y ~ x + (1 + x|group))
glmer(y ~ x + (1|group), family = 'binomial')
```

Note that if you're labeling your data correctly, it doesn't matter to lme4 whether the clustering is nested or crossed.  However if you give the nested clusters the same labels (e.g. 1,2,3,4 in every A, B, C), you'll need to note this.

```{r lme4_nest}
lmer(y ~ x + (1|group/nested_group))
lmer(y ~ x + (1|group) + (1|group:nested_group))
```

Again though, you shouldn't label your data such that the same label can refer to multiple things.




### lme4 extensions

The following is a graph of packages that have a connection to lme4 directly (thick lines) or indirectly, and gives a sense that you'll have a lot to work with should you use this package.  Zoom in to see package names.

```{r lme4_getgraph, eval=TRUE, echo=FALSE}
lme4_depgraph = get_depgraph('lme4')
```

```{r lme4_graph, echo=F, eval=T}
nodes = lme4_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = lme4_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

adj_mat = create_adjacency(edges, n1 = 'from', n2='to')

library(igraph)
deg = degree(graph.adjacency(adj_mat))
deg = data.frame(id = as.integer(names(deg))-1,  # for some reason something is zero
                 value=deg)
nodes = left_join(nodes, deg) %>% 
  mutate(value = ifelse(is.na(value), min(value, na.rm = T), value),
         value = value-min(value)+1)


visNetwork(nodes = nodes, edges=edges)
```


You'll note in the graph one of the relatively large nodes regards the car package. People use that because lme4 won't give you p-values.  You don't need them though, because you can just use confint to get the interval estimates, e.g. via bootstrap.  The merTools package provides a more efficient approach to interval estimation, and in my experience is more like what you'd get with a Bayesian approach.

Here are some other useful packages:

<span class="pack">merTools</span>: extracting useful information from and exploring the implications of <span class="objclass">merMod</span> objects 
<span class="pack">lmertest</span>: likelihood ratio and other tests
<span class="pack">flexmix</span>: mixture mixed models (e.g. latent class growth/trajectory)
<span class="pack">mediation</span>: mediation models with <span class="objclass">merMod</span> objects
<span class="basepack">mgcv</span>: additive mixed models (also via <span class="basepack">nlme</span>)
<span class="pack">brms</span>: Bayesian approach with the same syntax

## The mixed model object

Several mixed model packages have the same methods to extract certain features.

```{r mm_objects}
fixef(model)       # fixed effects coefficient
ranef(model)       # random effects coefficient
VarCorr(model)     # variance components
```


## Related models

You won't find much in the way of so-called <span class="emph">fixed effects models</span> (for panel data) in R, but the <span class="pack">plm</span> package does this .  The <span class="pack">geepack</span> will allow for <span class="emph">GEE</span> models to analyse data with various types of covariance structures.  And if all you want to do is correct your standard errors ('cluster-robust'), consider <span class="pack">sandwich</span>.

## Still other packages

We've mentioned <span class="pack">mgcv</span> (more detail later) and <span class="pack">ordinal.</span> One of the <span class="pack">lme4</span> developers is extending things to more complex models with <span class="pack">glmmTMB</span>.

## Mixed models list of packages

I have [several documents](http://m-clark.github.io/documents/#mixed-models) demonstrating mixed models for further exploration.


```{r mixed_percs, eval=TRUE, echo=FALSE}
nlme_perc = get_perc('nlme')
lme4_perc = get_perc('lme4')
plm_perc = get_perc('plm')
geepack_perc = get_perc('geepack')
sandwich_perc = get_perc('sandwich')
mgcv_perc = get_perc('mgcv')
ordinal_perc = get_perc('ordinal')
```


- <span class="basepack">nlme</span> `r nlme_perc`%
- <span class="pack">lme4</span> `r lme4_perc`%
- <span class="pack">plm</span> `r plm_perc`%
- <span class="pack">geepack</span> `r geepack_perc`%
- <span class="pack">sandwich</span> `r sandwich_perc`%
- <span class="pack">ordinal</span> `r ordinal_perc`%
- <span class="pack">mgcv</span> `r mgcv_perc`%

## Mixed models recommended reading

@gelman_arm

@wood_generalized_2017

@fahrmeir2013regression

@west2014linear

I have a document [here](https://m-clark.github.io/generalized-additive-models/).

# Additive models

<span class="emph">Generalized additive models</span> incorporate nonlinear, random, and spatial effects all under one roof.  One of the most powerful modeling packages in the R universe, the excellently named Mixed-GAM Computational Vehicle (<span class="pack">mgcv</span>), focuses on these, and even comes with the standard R installation!  

My own opinion is that GAMs should be one's baseline model, as they allow for more complex modeling but penalize that complexity to help guard against overfitting.  Furthermore, assuming a linear relationship for everything is overly simplistic at best.  GAMs also have close connections to mixed models, so can be seen as an extension of those.  You can find detailed examples of GAMs in my document [here](https://m-clark.github.io/generalized-additive-models).

```{r gam}
mgcv::gam(y ~ s(x))
gam(y ~ s(x), family = 'nb')                      # different family (negative binomial)
gam(y ~ s(x, k=20))                               # allow for more wiggles!
gam(y ~ s(x, by='group'))                         # different wiggles per group
gam(y ~ s(x, z))                                  # interaction
gam(y ~ s(x, bs='cc'))                            # alternate spline (cyclic cubic)
gam(y ~ s(group, bs='re'))                        # random effect with factor variable
gam(y ~ s(area, bs='mrf', xt=neighborhood_list))  # discrete spatial random effect
```

Once you run the model, you'll want to explore it.

```{r gam-explore}
gam.check(model)    # Did I let it get wiggly enough?
plot(model)         # Visualize effects
concurvity(model)   # the GAM form of collinearity
```



The package also comes with two means of modeling mixed effect models directly, with <span class="pack">nlme</span> support via the <span class="func">gamm</span> function, and the with <span class="pack">lme4</span> support via the <span class="pack">gamm4</span> package.  With the former one can also get at temporal autocorrelation as well via the `correlation` argument.  There is capability to deal with missing values, multivariate outcomes, and handle large data sets. If you don't use smooth terms, <span class="pack">mgcv</span> can simply be seen as a means to model additional distributional families.  


## Extensions and other considerations

It took a while, but people have finally started to realize the power of mgcv.

```{r mgcv_getgraph, eval=TRUE, echo=FALSE}
mgcv_depgraph = get_depgraph('mgcv')
```

```{r mgcv_graph, echo=F, eval=T}
# htmlwidgets fails with second graph. will display mixed model one but not this.
nodes = mgcv_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = mgcv_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

listed_packs = c('gamlss', 'VGAM', 'gamm4', 'ggplot2', 'gamair', 'flexmix', 'brms', 'mediation', 'pscl')
edges_trim = edges %>% 
  filter(to == 0) %>% 
  mutate(value = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 30L, value),
         length = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 0, 250))

nodes_trim = nodes %>% 
  filter(id %in% c(0, edges_trim$from)) %>% 
  mutate(value=c(100, rep(0, nrow(.)-1)),
         value = if_else(label %in% listed_packs, 30, value))
  
visNetwork(nodes = nodes_trim, 
             edges=edges_trim) %>% 
  visEdges(color=list(opacity=.10),
           # length = c(200,10),
           physics = T)
# adj_mat = create_adjacency(edges, n1 = 'from', n2='to')
# 
# library(igraph)
# deg = degree(graph.adjacency(adj_mat))
# deg = data.frame(id = as.integer(names(deg))-1,  # for some reason something is zero
#                  value=deg)
# nodes = left_join(nodes, deg) %>% 
#   mutate(value = ifelse(is.na(value), min(value, na.rm = T), value),
#          value = value-min(value)+1)
# 
# 
# visNetwork(nodes = nodes, edges=edges)
```



For example, most of the R community uses <span class="pack">ggplot2</span> to explore their data visually.  The package comes with <span class="func">geom_smooth</span>, which can actually use <span class="pack">mgcv</span> to display a nonlinear relationship between two variables.  Unfortunately it is only triggered automatically for large samples (> 1000), but you can always use it if desired via the `method` argument. You can even use the various arguments for the smooth term function.

```{r gam-ggplot}
ggplot2::ggplot(aes(x, y)) +
  geom_smooth(method = 'gam')

ggplot(aes(x, y)) +
  geom_smooth(method = 'gam', formula = y ~ s(x, bs='gp'))
```

I'll speak more about the package in the Bayesian section, but you can also use the <span class="func">s</span> function when using <span class="pack">brms</span>[^jagam].  Honestly, between those two packages and their respective capabilities, you'll likely need little more for your statistical modeling.  I personally use them all the time.

At one time both <span class="pack">gamlss</span> and <span class="pack">VGAM</span> extended GAMs beyond what <span class="pack">mgcv</span> could do, and they still have some modeling capabilities not seen in <span class="pack">mgcv</span>, e.g. other distributional families.  However, <span class="pack">mgcv</span> has since made up some of the more important differences.  Also, I still see some using the R-core <span class="func">spline</span> function and <span class="pack">splines</span> package.  There is nothing to be gained there however.  There is also the <span class="pack">gam</span> package, by Trevor Hastie, the person who wrote one of the most widely cited works on the technique.  While it certainly does the job, it's not on the level of <span class="pack">mgcv</span> in terms of what all it can do.  Unfortunately some packages that use GAMs use it instead of <span class="pack">mgcv</span>, so you won't have all the nice functionality.

In my document on GAMs I quote Shalizi who I think sums up the reason to use the models nicely. So I'll do so again here.

> With modern computing power, there are very few situations in which it is actually better to do linear regression than to fit an additive model. In fact, there seem to be only two good reasons to prefer linear models.<br> <br>
Our data analysis is guided by a credible scientific theory which asserts linear relationships among the variables we measure (not others, for which our observables serve as imperfect proxies).<br> <br>
Our data set is so massive that either the extra processing time, or the extra computer memory, needed to fit and store an additive rather than a linear model is prohibitive.<br> <br>
Even when the first reason applies, and we have good reasons to believe a linear theory, the truly scientific thing to do would be to check linearity, by fitting a flexible non-linear model and seeing if it looks close to linear. Even when the second reason applies, we would like to know how much bias we’re introducing by using linear predictors, which we could do by randomly selecting a subset of the data which is small enough for us to manage, and fitting an additive model. <br> <br>
In the vast majority of cases when users of statistical software fit linear models, neither of these justifications applies: theory doesn’t tell us to expect linearity, and our machines don’t compel us to use it. Linear regression is then employed for no better reason than that users know how to type lm but not gam. You now know better, and can spread the word.


```{r gam_percs, eval=TRUE, echo=FALSE}
splines_perc = get_perc('splines')
gamlss_perc = get_perc('gamlss')
VGAM_perc = get_perc('VGAM')
gam_perc = get_perc('gam')
splines_perc = get_perc('splines')
```


## Additive models list of packages

- <span class="pack">mgcv</span> `r mgcv_perc`%
- <span class="pack">gam</span> `r gam_perc`%
- <span class="pack">gamlss</span> `r gamlss_perc`%
- <span class="pack">VGAM</span> `r VGAM_perc`%
- <span class="pack">splines</span>  `r splines_perc`%

## GAM recommended reading

Feel free to peruse [my document](https://m-clark.github.io/generalized-additive-models/).

@wood_generalized_2017

@fahrmeir2013regression

# Survival analysis

<span class="emph">Survival analysis</span> is very common in biostatistics, epidemiology, public health etc.   It is also commonly called <span class="emph">event-history</span> analysis and <span class="emph">failure analysis</span> (engineering).    The basic idea is that you have a time-based target variable and want to model the time it takes until the event of interest occurs.  


Given the nature of the data you need two things to specify a target variable, the time counter and the indicator for whether the event of interest happened or not.

```{r survival_demo}
y = survival::Surv(time = t, event = q)  # standard right censored
y = Surv(t, t>0, type='left')            # left-censored
survival::coxph(y ~ x)                   # Cox Proportional Hazards Regression
cox.zph(model)                           # test proportional hazards assumption
anova(model)                             # anova summary table
coxph(y ~ x + strata(group))             # stratified
coxph(y ~ x + frailty.gaussian(group))   # random effect for group
coxph(Surv(start, stop, event) ~ x)      # time-dependent model
survreg(y ~ x, dist="exponential")       # parametric model
```


I've mentioned the <span class="pack">rms</span> package before.  It is written by Frank Harrell, a noted biostatistician who also contributed to SAS back in the day.  The package name is an acronym for the title of his book *Regression Modeling Strategies*, and given his discipline, he devotes a lot of content in the text, and functionality in the package, for survival analysis.  The book is a very good modeling book in general.  

```{r survival_rms}
y = survival::Surv(time = t, event = status)
dd = rms::datadist(x, z)             # set data  up for prediction
options(datadist='dd')
cph(y ~ x + z)                       # basic cox
psm(y ~ x + z, dist="weibull")       # parametric survival model
Mean(model)                          # create a function for further exploration
Survival(model)                      # create a function for further exploration
Quantile(model)                      # create a function for further exploration
ggplot(Predict(model, x, z))         # prediction plot
survplot(model)                      # survival plot
nomogram(model)                      # yep, nomogram
```

Along with the extensions that rms provides, you might also find something of use in packages like <span class="pack">Epi</span>, <span class="pack">epitools</span>, or <span class="pack">epiR</span>.  In addition, while the biostats world seems high on splines for their survival models (see e.g. `?rms::rcs`), and so you can incorporate them easily with the models above, likewise some of the additive models packages like <span class="pack">mgcv</span> have functionality for survival analysis.

```{r survival_gam}
mgcv::gam(time ~ s(x), weights = status, family = cox.ph)
```

And finally, you can find survival models in the machine learning context, e.g. with a package like <span class="pack">randomForestSRC</span>.

## Survival analysis list of packages

[Survival Task View](https://www.rdocumentation.org/taskviews#Survival)

```{r survival_percs, eval=TRUE, echo=FALSE}
survival_perc = get_perc('survival')
rms_perc = get_perc('rms')
randomForestSRC_perc = get_perc('randomForestSRC')
Epi_perc = get_perc('Epi')
epitools_perc = get_perc('epitools')
epiR_perc = get_perc('epiR')
```

- <span class="basepack">survival</span> `r survival_perc`%
- <span class="pack">rms</span> `r rms_perc`%
- <span class="pack">randomForestSRC</span>  `r randomForestSRC_perc`%
- <span class="pack">Epi</span>  `r Epi_perc`%
- <span class="pack">epitools</span>  `r epitools_perc`%
- <span class="pack">epiR</span>  `r epiR_perc`%
- <span class="pack">mgcv</span> `r mgcv_perc`%


## Survival analysis recommended reading

@harrell2015

# Survey weighting

Many data sets in the social sciences are the result of a <span class="emph">survey design</span>, where certain geographic regions and populations are sampled in precise ways.  With the survey weights, one can make more appropriate inferences to the population from which the data was drawn.

I don't have a lot of experience here, but can at least show a demo and provide some resources.  The first issue is setting up the survey design.  Your data will presumably have the weights and other necessary information (e.g. the finite population correction).

```{r survey_glm}
# no clusters, stratified, with finite populaton correction
dstrat = survey::svydesign(id=~1, strata=~stype, weights=~pw, fpc=~fpc)

dclus = svydesign(id=~clus, weights=~pw)       # clustered design

svymean(~y, dclus, deff=TRUE)                  # survey weighted mean
svytotal(~z, dclus, deff=TRUE)                 # survey weighted total
svyratio(numerator = ~y, denominator= ~z, dclus, deff=TRUE)    # survey weighted ratio
svyglm(y ~ x, data=dstrat, family='binomial')  # standard glm
```

There are also some other models like survival models, factor analysis, and more.  One thing I can also mention- don't use the `weights` argument in the base R <span class="func">lm/glm</span> functions with survey weights.  That argument refers to a different type of weight (inverse variance).


## Survey analysis list of packages

```{r survey_percs, eval=TRUE, echo=FALSE}
survey_perc = get_perc('survey')
```

- <span class="pack">survey</span> `r survey_perc`%

## Survey analysis recommended reading

@lumley2011complex

@heeringa2017applied

# Principal components & Factor analysis

A lot of techniques fall under the heading of 'dimension reduction', 'factor analysis', or 'matrix factorization' techniques.  


## PCA

Principal components analysis is one of the most widely used dimension reduction techniques, and brings us to the first example of unsupervised methods.  In this case, we don't have a single target variable to predict, but several, and generally we are not modeling them with other covariates, but trying to reduce them.  The basic idea of PCA is to reduce the data to fewer dimensions that nevertheless retain the most variance seen in the data.

Base R has <span class="func">princomp</span> and <span class="func">prcomp</span> functions, but they merely get the job done.  There are plenty of extensions of PCA to supervised settings, but you can do that exploration on your own.  

```{r pca_demo}
X = scale(X)          # standardize the data
princomp(X)           # via eigen decomposition
prcomp(X)             # via singular value decomposition
```

Beyond that you'll have some methods like <span class="func">loadings</span>, and these will often work in other packages as well.  The most common things you'll want are scores and loadings.

```{r pca_methods}
X$scores(X)           # component scores
X$loadings(X)         # loadings/weights
loadings(X)
```

Note that this recommendation comes from someone who has little use for PCA (but very often does factor analysis), I can recommend a couple other packages.  The <span class="pack">psych</span> package puts PCA more squarely in the framework of factor analysis more generally, and offers rotations, other measures of fit, etc.  The <span class="pack">pcaMethods</span> package provides probabilistic PCA, Bayesian PCA, cross-validation,  selection of the number of components, PCA-based imputation, and more.

```{r pca_extend}
model = psych::principal(X, nfactors = 2, rotate='varimax')
model$loadings
model$scores

model = pcaMethods::ppca(X, nPcs = 2)      # probabilistic PCA
loadings(model)
scores(model)
```

There are plenty of supervised extensions to PCA, but generally you'll have better choices.  For example, instead of PC regression, use techniques that won't require the dimension reduction, would better to such a process without being as restrictive (like a neural network), or would better suit the goals of some analyses (e.g. when variables belong to the same scale).

## Factor analysis

The base R approach to the most common form of factor analysis is pretty much identical to PCA.  

```{r fa_demo}
model = factanal(X, factors = 2, scores = 'regression', rotation = 'promax')
model$loadings
model$scores
```


The <span class="pack">psych</span> package offers far, far, more in this realm.  The output alone will take a bit for you to get through.

```{r fa_psych}
model = psych::fa(X, nfactors = 2, rotate = 'oblimin', fm = 'ml')  # via maximum likelihood
model$loadings
model$scores
fa.diagram(model)                     # visualize
predict(model, newdata=mynewdata)     # estimate scores for new data
nfactors(X, 4, rotate='promax')       # compare solutions for 1:4 factors
```

Once you get into latent variable models like factor analysis, you'll have plenty of tools at your disposal.  Check out <span class="pack">lavaan</span> (confirmatory factor analysis/SEM) and <span class="pack">ltm</span> (item response theory models) for starters.

## PCA/FA list of packages

[Psychometrics Task View](https://www.rdocumentation.org/taskviews#Psychometrics)

```{r pcafa_percs, eval=TRUE, echo=FALSE}
psych_perc = get_perc('psych')
pcaMethods_perc = get_perc('pcaMethods')
lavaan_perc = get_perc('lavaan')
ltm_perc = get_perc('ltm')
```

- <span class="pack">psych</span> `r psych_perc`%
- <span class="pack">pcaMethods</span> (not accurately tracked since on BioConductor)
- <span class="pack">lavaan</span> `r lavaan_perc`%
- <span class="pack">ltm</span> `r ltm_perc`%


## PCA/FA recommended reading

I have some more on many dimension reduction techniques [here](https://m-clark.github.io/sem/FA_notes.html), which briefly covers/demos a wide variety of techniques like t-sne, non-negative matrix factorization, latent Dirichlet allocation, mixture models, recommender systems and more.  It spends more time on PCA and FA.

I also cover PCA/FA [here](https://m-clark.github.io/sem/), along with SEM, mixture models, IRT models and more.

For measurement in general, consider psychometrician and <span class="pack">psych</span> package author [William Revelle's freely available text](http://www.personality-project.org/r/book/).  You will also learn a ridiculous amount by simply reading the help files to the various functions in his package.

More to follow in the [SEM][Structural Equation Modeling] section.



# Mixture models and cluster analysis

Often the goal is to find latent structure (or simply dimension reduction) among the observations (rows), as opposed to the variables (columns).  As such we have another unsupervised modeling situation that is very commonly used- cluster analysis.  

## Mixture models

We'll start with model-based approaches to the problem, typically referred to as <span class="emph">mixture models</span>. A good starting point here is <span class="pack">mclust</span>.  It provides a very easy way to search across a wide variety of cluster types, as well as any number of clusters (including 1 cluster).  You may then select the 'best' solution based on BIC, with is the advantage of a model-based approach.  Furthermore, it provides plots of the search space, the classifications, uncertainty and density plots.

```{r cluster_mclus}
model = mclust::Mclust(X, G = 1:4)
summary(model)
plot(model)
```

Often people want to do a cluster analysis and then use those clusters in a subsequent analysis, e.g. as a predictor.  This two-stage approach fails to take into account the uncertainty in the class assignment.  However, the mixture model approach can do this appropriately, and we have another long-standing R package to help us there- <span class="pack">flexmix</span>.  This package can actually handle a wide variety of regression models (e.g. different response distributions, mixed models[^lgc]), allowing the results to vary across the latent clusters.  It uses S4 class objects.

```{r cluster_flexmix}
model = flexmix::flexmix(yn ~ x, k = 2)  # two clusters
summary(model)                    # basic info
parameters(model)                 # coefficients (for each cluster/class)
posterior(model)                  # probability of class membership

# two outcomes, two models, two classes
model = flexmix(yn ~ x, k = 2,
                model = list(FLXMRglm(yn ~ x + z), 
                             FLXMRglm(yp ~ x + offset(q), family = 'poisson')))
```


Another type of model, or at least term, you may here with regard to mixture models is <span class="emph">latent class analysis</span>.  The only distinction here is that the data is categorical.  The <span class="pack">poLCA</span> package focuses on this.  It also will allow for modeling the latent classes as an outcome (in distinction to <span class="pack">flexmix</span>), just as you would in standard logistic regression settings.

```{r cluster_polca}
# assume X is a matrix of binary indicator variables
model = poLCA::poLCA(X ~ 1, values, nclass = 2)      # basic cluster analysis
model = poLCA(X ~ x + z, values, nclass = 2)         # add predictors
poLCA.posterior(model)                               # probability of class membership
```

While it does about as much as many would want, unfortunately <span class="pack">poLCA</span> is pretty bare bones. Look for <span class="pack">lavaan</span> to add latent classes in the future.

When it comes to modeling with latent clusters, there's a lot of ways to do so.  A very common approach that one sees in other modeling frameworks is hidden Markov models.  Consider the <span class="pack">hmm</span> package as a starting point, but there's a lot out there.

## Traditional cluster analysis

For traditional cluster analysis based on distance matrices, one doesn't really need to go beyond the base R installation, as it comes with <span class="func">dist</span>, <span class="func">kmeans</span>, <span class="pack">hclust</span> functions, and the <span class="func">recommended</span> package cluster.

```{r cluster_traditional}
X_dist = dist(X)                          # euclidean distances
X_dist = dist(X, method = 'manhattan')    # manhattan distance

kmeans(X_dist, 3)                         # k-means 3 cluster solution

hclust(X_dist, method = 'average')        # hierarchical clustering with average linkage
cluster::agnes(X_dist)                    # agglomerative clustering
cluster::diana(X_dist)                    # divisive clustering
```

Unfortunately such methods come with many arbitrary choices- distance metric, linkage method, number of clusters, clustering method, 'performance', etc.  I'd suggest something like <span class="pack">clusterSim</span> to make it easy to search through the minimally dozens of options you might consider.  For example, the following would search over several clustering approaches, evaluating them across different cluster numbers, cluster quality assessments, distance metrics, normalization techniques and so forth.

```{r cluster_sim}
clusterSim::cluster.Sim(X, 
                        p = 1,
                        minClusterNo = 2,
                        maxClusterNo = 5,
                        icq = c('G1', 'S'),
                        distances = c('d2', 'd5'),
                        normalizations = c('n1', 'n3'),
                        methods = c('m5', 'm3', 'm1'))
```


## Cluster analysis list of packages

```{r cluster_percs, eval=TRUE, echo=FALSE}
Mclust_perc = get_perc('Mclust')
flexmix_perc = get_perc('flexmix')
hmm_perc = get_perc('hmm')
polca_perc = get_perc('polca')
clusterSim_perc = get_perc('clusterSim')
cluster_perc = get_perc('cluster')
```


- <span class="pack">mclust</span> `r Mclust_perc`%
- <span class="pack">flexmix</span> `r flexmix_perc`%
- <span class="pack">hmm</span> `r hmm_perc`%
- <span class="pack">polca</span> `r polca_perc`%
- <span class="pack">clusterSim</span> `r clusterSim_perc`%
- <span class="basepack">cluster</span> `r cluster_perc`%

## Cluster analysis recommended reading

[Cluster Analysis Task View](https://www.rdocumentation.org/taskviews#Cluster)

@everitt2011cluster Provides an overview of traditional methods in a clean manner.  Can't speak much to more modern references.

I have a bit on mixture models and latent class analysis from the SEM perspective [here](https://m-clark.github.io/sem/mixture-models).




# Structural Equation Modeling

<span class="emph">Structural Equation Modeling</span> (SEM) is a very broad technique that encompasses and extends many others, bringing together regression, factor analysis, mixtures models and more. In particular it combines measurement models regarding latent variables and observed variables, and structural models to explore the effects of latent and observed variables on others.

Due to the use of latent variables and other specifics, a special syntax is required to run SEM models, but otherwise it does not require much to get going, at least with the <span class="pack">lavaan</span> package.  For those familiar with Mplus, it and various extensions can do about 90% of what Mplus does[^lavlack], and even spits out results in the same format.

## Path Analysis

We can start with a <span class="emph">path analysis</span> using only observed variables.  If you know how to run a regression, you can do this easily.

```{r path_lavaan}
model = "
  y ~ x + q + r + z
  z ~ x + q
"

result = lavaan::sem(model)
summary(result, standardized=TRUE)  # result with standardized effects
```

## Confirmatory Factor Analysis

<span class="emph">Confirmatory factor analysis</span> is just like any other factor analysis, except some loadings are constrained to be zero, usually for theoretical and other not so good reasons.

```{r cfa_lavaan}
model = "
  lv1 =~ x1 + x2 + x3
  lv2 =~ y1 + y2 + y3

  y1 ~~ x1     # residual correlation to be estimated
"

result = cfa(model)
lavPredict(result)   # get factor scores
```

## SEM

SEM allows us to measure multiple outcomes, indirect effects, and combine latent and observed variables, all at once.

```{r sem_lavaan}
model = "
  lv1 =~ x1 + x2 + x3
  lv2 =~ y1 + y2
  lv3 =~ z1 + z2 + z3
  
  # a mediation model with lv2 as mediator
  lv3 ~ lv1 + lv2 + q + f    # q and f are observed
  lv2 ~ lv1 + q
"

# sem with an approach to deal with missing values, robust ML, additional fit measures
sem(model, missing='fiml', estimator='MLR', fit.measures=TRUE)  
```

### lavaan extensions

Several packages extend the capabilities of <span class="pack">lavaan</span>. 

```{r lavaan_getgraph, eval=TRUE, echo=FALSE}
lavaan_depgraph = get_depgraph('lavaan')
```

```{r lavaan_graph, echo=F, eval=T, cache=FALSE}
nodes = lavaan_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = lavaan_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

listed_packs = c('blavaan', 'survey.lavaan', 'lavaanPlot', 'semTools', 'regsem', 'lavaan.shiny', 'psych', 'piecewiseSEM', 'AER')
edges_trim = edges %>% 
  filter(to == 0) %>% 
  mutate(value = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 30L, value),
         length = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 0, 250))

nodes_trim = nodes %>% 
  filter(id %in% c(0, edges_trim$from)) %>% 
  mutate(value=c(100, rep(0, nrow(.)-1)),
         value = if_else(label %in% listed_packs, 30, value))
  
visNetwork(nodes = nodes_trim, 
             edges=edges_trim) %>% 
  visEdges(color=list(opacity=.10),
           # length = c(200,10),
           physics = T)
```

In terms of Mplus style extensions, note <span class="pack">lavaan.survey</span> (survey design), <span class="pack">blavaan</span> (Bayesian), and <span class="pack">semTools</span>, which adds things like multiple imputation, measurement invariance testing, additional fit indices and a lot more.

A couple others I will note but with which I haven't used yet.  I note them because I will at some point, and their intentions are in the right place. One is <span class="pack">regsem</span>.  The vast majority of SEM models are grossly overfit, estimating dozens of parameters with only a couple hundred observations.  Such a package can surely assist with this problem, though I predict many will not like what they find (even though it will be a better result).  In addition, while writing this I came across the <span class="pack">lavaanPlot</span> package, which might save me at least 50% of the headache of using <span class="pack">Diagrammer</span>.

### Beyond lavaan

Going beyond lavaan, I would suggest looking into the <span class="pack">mediation</span> package if you only have observed variables, as it can handle a wide variety of models for the mediator and outcome.  In a similar vein check out <span class="pack">piecewiseSEM</span>. <span class="pack">MplusAutomation</span> will save you from having to use Mplus even if you want to use it.  With one template you can write the syntax files, run the models, and bring back the results into R in a usable form, and all automatically, without ever opening Mplus.

Others that might be of note but I can't speak to include <span class="pack">lava</span> and <span class="pack">OpenMx</span>.  In addition, the aforementioned <span class="pack">psych</span> package has some SEM/lavaan capabilities.


## SEM analysis list of packages

```{r sem_percs, eval=TRUE, echo=FALSE}
lavaan_perc = get_perc('lavaan')
lavaan_survey_perc = get_perc('lavaan.survey')
blavaan_perc = get_perc('blavaan')
lavaanPlot_perc = get_perc('lavaanPlot')
semTools_perc = get_perc('semTools')
regsem_perc = get_perc('regsem')
mediation_perc = get_perc('mediation')
piecewiseSEM_perc = get_perc('piecewiseSEM')
MplusAutomation_perc = get_perc('MplusAutomation')
lava_perc = get_perc('lava')
OpenMx_perc = get_perc('OpenMx')
psych_perc = get_perc('psych')
```


- <span class="pack">lavaan</span> `r lavaan_perc`%
- <span class="pack">lavaan.survey</span> `r lavaan_survey_perc`%
- <span class="pack">blavaan</span> `r blavaan_perc`%
- <span class="pack">lavaanPlot</span> `r lavaanPlot_perc`%
- <span class="pack">semTools</span> `r semTools_perc`%
- <span class="pack">regsem</span> `r regsem_perc`%
- <span class="pack">mediation</span> `r mediation_perc`%
- <span class="pack">piecewiseSEM</span> `r piecewiseSEM_perc`%
- <span class="pack">MplusAutomation</span> `r MplusAutomation_perc`%
- <span class="pack">lava</span> `r lava_perc`%
- <span class="pack">OpenMx</span> `r OpenMx_perc`%
- <span class="pack">psych</span> `r psych_perc`%


## SEM recommended reading

[Psychometrics Task View](https://www.rdocumentation.org/taskviews#Psychometrics)

My doc on SEM covers a lot of ground and provides more extensive examples.

@kline2015principles is a very popular applied treatment. See the classic @bollen1989 for more details regarding standard models.



# Graphical models

SEM can be seen as a very large subset of what are generally called <span class="emph">graphical models</span>, where variables are represented by <span class="emph">nodes</span> (sometimes called vertices) and the relations among them are represented by <span class="emph"></span> (sometimes called arcs).  Edges may be <span class="emph">directed</span> (i.e. have arrows) or <span class="emph">undirected</span>.  With directed (acyclic) graphs (DAG), we get into things like causal modeling, SEM, bayesian networks, etc. With undirected graphs, often called a Markov random fields, applications are in network models, spatial analysis, and more.

In most cases there are two main approaches to running such models.  One is where two data sets are provided, one which specifies the nodes and various attributes associated with them. In addition, one has an edge list or data frame that specifies the nodes from which a connection begins, the nodes to which they connect, and possibly a column specifying the value/weight for the connection.  The other approach regards an <span class="emph">adjacency matrix</span>, which is like a correlation or (inverse) distance matrix, where the off-diagonal values specify connections among the nodes, and may be simply binary or comprise continuous <span class="emph">weights</span>.

One of the more popular and powerful packages is <span class="pack">igraph</span>, and even when you're using others, they are often using it under the hood. Given the right data, one can create a graph object, and then get any number of statistics, visualize it, or whatever.  A lot of it is for the creation of graphs, but normally graphs are too big to do by hand and/or you'd have the data already, and it'd be easier to create an adjacency matrix from your typical data frame and work from there.

```{r gm_igraph, echo=-1}
# g = igraph::make_graph("Zachary")  # a social network
g = igraph::from_adjacency(adjmat)      # create graph from adjacency matrix
g = from_data_frame(df)                 # create graph from a data frame
as_adjacency_matrix(g)                  # convert graph to adjacency
plot(g)                                 # plot it
degree(g)                               # degree for each node
hub_score(g)                            # hub score for each node
count_triangles(g)                      # N triangles for each node
clusters = cluster_edge_betweenness(g)  # detect clusters
membership(clusters)
communities(clusters)
write_graph(g, 'g.dot')                 # write graph to e.g. dot format
```

The basic plot functionality for <span class="pack">igraph</span> is... basic.  It's not worth your time to make it pretty.  To that end I suggest packages like <span class="pack">networkd3</span>, <span class="pack">visNetwork</span>, and <span class="pack">Diagrammer.</span>  It's not too big a deal to convert the graph object to something they will use (<span class="pack">visNetwork</span> will even work with an igraph object, e.g. with <span class="func">toVisNetworkData</span>).

### Bayesian networks

A package that's useful for <span class="emph">bayesian networks</span>, or probabilistic DAG, is <span class="pack">bnlearn.</span>  In this case we are looking for a graphical structure based on the data we have. The package allows for model fitting such that you could essentially duplicate a path analysis SEM-style, but the strength is in the model search and selection via a variety algorithms, possibly with cross-validation.  It can work with binary, continuous, or a mix of variables.  With blacklists and whitelists, one can impose theoretical structure on the search such that some connections are always present while others are forbidden.

```{r gm_bnlearn}
model = bnlearn::gs(X, blacklist = bl)   # grow-shrink algorithm with a blacklist
plot(model)
```

### Network analysis

<span class="emph">Network analysis</span> employs graphical models to study things like social or other specific types of networks.  Two primary packages are <span class="pack">network</span>, which like <span class="pack">igraph</span>, serves as a foundation package, and <span class="pack">sna.</span>  The primary object is a network class object instead of a graph object like with <span class="pack">igraph</span>, but you more or less proceed similarly.  

```{r gn_sna}
g = network::network(adj_mat, directed=FALSE)   # undirected network based on adjacency matrix
sna::summary(g)                                 # use sna for the rest
plot(g)
betweenness(g)                                  # various network statistics
degree(g)
efficiency(g)
model = ergm::ergm(g ~ edges)                   # exponential random graph model
```

With <span class="pack">sna</span> you may have some specific statistics not found in <span class="pack">igraph</span>, but it's not really an either or, as you can easily move between the two. As with <span class="pack">igraph</span>, you'll probably want to use a different package for visualization.  While <span class="pack">sna</span> has some modeling capabilities, you might also consider <span class="pack">ergm</span> for exponential random graph models.


## Graphical models list of packages

```{r gm_percs, eval=TRUE, echo=FALSE}
igraph_perc = get_perc('igraph')
bnlearn_perc = get_perc('bnlearn')
network_perc = get_perc('network')
sna_perc = get_perc('sna')
ergm_perc = get_perc('ergm')
networkd3_perc = get_perc('networkd3')
visnetwork_perc = get_perc('visNetwork')
Diagrammer_perc = get_perc('Diagrammer')
```


- <span class="pack">igraph</span> `r igraph_perc`%
- <span class="pack">bnlearn</span> `r bnlearn_perc`%
- <span class="pack">network</span> `r network_perc`%
- <span class="pack">sna</span> `r sna_perc`%
- <span class="pack">ergm</span> `r ergm_perc`%
- <span class="pack">networkd3</span> `r networkd3_perc`%
- <span class="pack">visNetwork</span> `r visnetwork_perc`%
- <span class="pack">Diagrammer</span> `r Diagrammer_perc`%



## Graphical models recommended reading

[Graphical Model Task View](https://www.rdocumentation.org/taskviews#gR)

@pearl2009causality Also, he has every article available on [his website](http://bayes.cs.ucla.edu/jp_home.html).

There are several Use R! series entries for graphical models and network analysis.

- @hojsgaard2012graphical
- @luke2015network
- @kolaczyk2014network
- @nagarajan2013bnetwork


# Time series

With time series data, there isn't really a specific model of concern, and in general, many models you might want to run can't do anything special with a time-series class object.  For example, for a specific situation with observations over time, one could run a mixed model, a GAM, a gaussian process model, an ARIMA model, hidden markov model, or a recurrent neural network.  However, there is concern about how to deal with the data itself even for basic processing and visualization.

To begin, note that there is a lot in the base R installation for dealing with time-based objects.

```{r ts_funcs}
acf(Series)              # autocorrelation plot
lag(Series, 1)           # can be used in a formula
diff(Series, lag = 2)    # take differences
fft(Series)              # fast fourier transform


Series.ts = as.ts(Series)
plot(Series.ts)
frequency(Series.ts)
```

A couple very useful packages that make dealing with time and dates are <span class="pack">xts</span>, <span class="pack">zoo</span>, and <span class="pack">lubridate</span>.

```{r ts_lubridate}
xts::as.xts(Series)
apply.monthly(Series, somefunction)  # apply a function at a specific period

zoo::window(Series)

lubridate::as_date(myDates)
ymd(myDates)                # convert to YYYY-MM-DD format
hour(myDates)               # Extract hours
```

For modeling, again that depends on many things and may just be one of many models already demonstrated.  I'll mention <span class="pack">forecast</span> as a means to get started with traditional models like auto-regressive moving average and state-space models (with tidy piping functionality), and <span class="pack">prophet</span>, an offering by Facebook that uses Stan (see Bayesian section).

```{r ts_models}
forecast::auto.arima(Series) %>% plot()

# Series.df is a data.frame with two columns of date ('ds') and value ('y')
model = prophet::prophet(Series.df, growth = 'linear', n.changepoints = 1)
model$changepoints                                 # date of estimated changepoint
forecast = predict(model, Series.df)
plot(model, fcst=forecast)
```

The following demonstrates an additive mixed model with a cyclic spline for the trend, and which models the residuals with an autoregressive structure.

```{r ts_gam}
mgcv::gamm(y ~ s(time, bs='cc'), correlation = corAR1())
```

Again the main thing is just to know that tools are available to help process time series, but models may not require it.  The <span class="pack">prophet</span> package expects a date object, whereas <span class="pack">mgcv</span> did not.



## Time series list of packages

```{r ts_percs, eval=TRUE, echo=FALSE}
xts_perc = get_perc('xts')
zoo_perc = get_perc('zoo')
forecast_perc = get_perc('forecast')
prophet_perc = get_perc('prophet')
lubridate_perc = get_perc('lubridate')
```


- <span class="pack">xts</span> `r xts_perc`%
- <span class="pack">zoo</span> `r zoo_perc`%
- <span class="pack">forecast</span> `r forecast_perc`%
- <span class="pack">prophet</span> `r prophet_perc`%
- <span class="pack">lubridate</span> `r lubridate_perc`%


## Time series recommended reading

[Time Series Task View](https://www.rdocumentation.org/taskviews#TimeSeries)


# Spatial models

Spatial models incorporate additional structure in a model that has some spatial component, usually, but not limited to, geography.  A lot of the work you'll do is with the data processing rather than modeling part.  

A starting point for spatial data is the <span class="pack">sp</span> package, which provides basic spatial objects that can then be used for mapping or modeling, such as <span class="objclass">SpatialPoints</span>, <span class="objclass">SpatialPolygonsDataFrame</span> and more.  Many, many other packages rely on <span class="pack">sp</span>.


```{r sp_getgraph, eval=TRUE, echo=FALSE}
sp_depgraph = get_depgraph('sp')
```

```{r sp_graph, echo=F, eval=T, cache=FALSE}
nodes = sp_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = sp_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

listed_packs = c('sf', 'maps', 'maptools', 'spdep', 'mapview', 'ggplot2', 'leaflet', 'spacetime', 'ggmap', 'rgeos')
edges_trim = edges %>% 
  filter(to == 0) %>% 
  mutate(value = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 30L, value),
         length = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 0, 250))

nodes_trim = nodes %>% 
  filter(id %in% c(0, edges_trim$from)) %>% 
  mutate(value=c(100, rep(0, nrow(.)-1)),
         value = if_else(label %in% listed_packs, 30, value))
  
visNetwork(nodes = nodes_trim, 
             edges=edges_trim) %>% 
  visEdges(color=list(opacity=.10),
           # length = c(200,10),
           physics = T)
```



A lot of <span class="pack">sp</span>, and its corresponding Use R! book, is geared toward creating a map from scratch, which is great, but often rarely needed, as typically one already has map data, e.g. in the form of shape files, and wants to simply import and then model or visualize it.  It also uses lattice and base R for visualization, and is inordinately slow to visualize its products.   While sp has been around for a long time, the newer <span class="pack">sf</span> ([simple features](https://en.wikipedia.org/wiki/Simple_Features)) package will eventually supersede it (or at least that's the plan), so I'll give a quick example.

```{r spatial_sf}
map_obj = sf::st_read("Shapefile.shp")
plot(map_obj)
head(map_obj)         # spatial data.frame

world1 = st_as_sf(maps::map('world', plot = FALSE, fill = TRUE))  # convert maps object to sf
ggplot2::ggplot() + geom_sf(data = world1)                        # plot with ggplot2
```


The <span class="pack">spdep</span> package can serve as a first step to spatial modeling, providing common spatial descriptive tests and models.  However, functionality may be available in other packages that does not require specific spatial data structures.

```{r spatial_spdep}
nb = spdep::nb2listw(nb_object, style="W")             # create a weigthed neighbor list
moran(y, list=nb, length(nb_object), Szero(nb))        # Moran's I
moran.test(y, nb)                                      # Moran test for spatial autocorrelation

model = spautolm(y ~ x + z, listw=nb, family="CAR")    # conditional autoregressive model
summary(model)

model = lme(y ~ x, correlation=corSpatial(~ lat + lon, type='gaussian'))   # via mixed model
model = gam(y ~ s(lat + lon, bs='gp'))                         # via additive model
model = gam(y ~ s(x) + s(geog_units, bs='mrf', xt=polygons))   # discrete space (markov random field)
```

### Other tools

The <span class="pack">maptools</span> package allows for easy conversion and combining of <span class="pack">sp</span> objects, and provides additional useful functions. For modeling, Bayesian approaches, like those that [R-INLA](http://www.r-inla.org) provides, are common in the spatial realm.

For visualization, use <span class="pack">maps</span> for a quick peek and subsequent conversion to <span class="pack">sp</span> object (via maptools). For pretty (and often pretty easy) visualizations,  geom_sf (in ggplot2), <span class="pack">ggmap</span>, <span class="pack">mapview</span>, <span class="pack">leaflet</span>, and <span class="pack">plotly</span>.

Note also that there are a lot of packages being used under the hood, e.g. <span class="pack">rgeos</span>, <span class="pack">rgdal</span>, but which you won't use directly. In general, spatial tools, especially for visualization, have come a long way relatively recently, so you'll best want to investigate to see what's out there regularly.  

## Spatial list of packages


```{r spatial_percs, eval=TRUE, echo=FALSE}
sp_perc = get_perc('sp')
sf_perc = get_perc('sf')
spdep_perc = get_perc('spdep')
maptools_perc = get_perc('maptools')
maps_perc = get_perc('maps')
mapview_perc = get_perc('mapview')
leaflet_perc = get_perc('leaflet')
plotly_perc = get_perc('plotly')
ggmap_perc = get_perc('ggmap')
rgeos_perc = get_perc('rgeos')
rgdal_perc = get_perc('rgdal')
```


- <span class="pack">sp</span> `r sp_perc`%
- <span class="pack">sf</span> `r sf_perc`%
- <span class="pack">spdep</span> `r spdep_perc`%
- <span class="pack">maptools</span> `r maptools_perc`%
- <span class="pack">maps</span> `r maps_perc`%
- <span class="pack">mapview</span> `r mapview_perc`%
- <span class="pack">leaflet</span> `r leaflet_perc`%
- <span class="pack">plotly</span> `r plotly_perc`%
- <span class="pack">ggmap</span> `r ggmap_perc`%
- <span class="pack">rgeos</span> `r rgeos_perc`%
- <span class="pack">rgdal</span> `r rgdal_perc`%


## Spatial recommended reading

[Spatial Task View](https://www.rdocumentation.org/taskviews#Spatial)

[SpatialTemporal Task View](https://www.rdocumentation.org/taskviews#SpatioTemporal)

@bivand2013applied  Not sure how useful this still is as far as how you'd want to go about things these days, but you would likely learn a lot.


# Machine learning

Machine learning is more of an approach than a specific set of analysis techniques, though one will find models they won't see in traditional statistical modeling.  For practical purposes, your approach will involve measuring performance on a test (validation, hold-out) set of data rather than the data you fit the model on.

Two packages are very popular in this realm, <span class="pack">caret</span> and <span class="pack">mlr</span>, but they work similarly because again, ML is more of a conceptual approach than a technique. We'll demonstrate both on a classification task with a random forest.  


```{r ml_caret}
ind = caret::createDataPartition(y, p = .8)      # index to create a training and test set
cv_opts = trainControl(method='cv', number=10)   # 10-fold cross validation

# random forest with various options
model_train  = train(X_train, y_train, 
                     method = 'rf', 
                     tuneLength=5, 
                     trControl = cv_opts,
                     preProcess = c("center", "scale"))  

preds_rf = predict(model_train, X_test)          # predictions
confusionMatrix(preds_rf, y_test)                # classification results
```

If you use caret for nothing else, the <span class="func">confusionMatrix</span> function is very handy for any classification model.

Here is a similar approach using <span class="pack">mlr.</span>

```{r ml_mlr}
task = mlr::makeClassifTask(data = X, target = "y")   # X is a data.frame with all variables

n = nrow(X)
train.set = sample(n, size = .8*n)                    # indices for training data
test.set = setdiff(1:n, train.set)                    # indices for test data

lrn = makeLearner("classif.randomForest")

model = train(lrn, task, subset = train.set)

preds_rf = predict(model, task = task, subset = test.set)

calculateConfusionMatrix(preds_rf, relative = T)
```

Both packages offer a very wide variety of ways to tune and customize your approach, and offer literally hundreds of models between them.  Some other packages that offer some specific models (accessible by <span class="pack">caret</span> or <span class="pack">mlr</span> if desired) that one might want to be aware of are <span class="pack">e1071</span>, <span class="pack">glmnet</span>, <span class="pack">randomForest</span>, and <span class="pack">xgboost</span>.  For understanding such models, consider <span class="pack">lime</span>.


## Machine Learning list of packages

```{r ml_percs, eval=TRUE, echo=FALSE}
caret_perc = get_perc('caret')
mlr_perc = get_perc('mlr')
e1071_perc = get_perc('e1071')
glmnet_perc = get_perc('glmnet')
randomForest_perc = get_perc('randomForest')
xgboost_perc = get_perc('xgboost')
lime_perc = get_perc('lime')
```


- <span class="pack">caret</span> `r caret_perc`%
- <span class="pack">mlr</span> `r mlr_perc`%
- <span class="pack">e1071</span> `r e1071_perc`%
- <span class="pack">glmnet</span> `r glmnet_perc`%
- <span class="pack">randomForest</span> `r randomForest_perc`%
- <span class="pack">xgboost</span> `r xgboost_perc`%
- <span class="pack">lime</span> `r lime_perc`%


## Machine Learning recommended reading

[Machine Learning Task View](https://www.rdocumentation.org/taskviews#MachineLearning)

@hastie_elements_2009  freely available [here](https://web.stanford.edu/~hastie/ElemStatLearn/)

@james2013introduction

@efron2016computer

@murphy_machine_2012

@kuhn2013applied   Book by the caret package author

You can find detailed examples in my document [here](https://m-clark.github.io/introduction-to-machine-learning).


# Bayesian

Bayesian modeling is very easy to do in R these days, even for fairly complicated models.  Many packages use BUGS or its JAGS dialect, though more recent developments have been made with Stan.

## Miscellaneous packages

Bayesian analysis has a long history in R, so you'll find various packages more or less doing their own thing, but revolving around a couple of key packages like <span class="pack">coda</span>, <span class="pack">MCMCpack</span>, <span class="pack">rjags</span>, etc.  You can use <span class="pack">rjags</span> to run something written in BUGS and get further processing of the results with <span class="pack">coda.</span>  <span class="pack">MCMCpack</span> and <span class="pack">MCMCglmm</span> provide the ability to run several types of models in the Bayesian context.

```{r bayes_misc}
model <- rjags::coda.samples(X, c("alpha","beta","sigma"), n.iter=1000)
summary(model)

coda::gelman.diag(model)    # a diagnostic
coda::traceplot(model)      # a diagnostic

MCMCpack::MCMClogit(y ~ x + z, burnin = 1000, mcmc = 5000, thin = 10)  # logistic regression model
MCMCglmm::MCMCglmm(PO ~ x + z, burnin = 1000, nitt = 5000, thin = 10, random =  ~group)  # a mixed model
```


## Stan

Many of those packages are seeing decreased use because of <span class="emph">Stan</span>. Stan is a probabilistic programming language developed by a very smart group of people at Columbia, and whose development has very rapidly enabled many people to jump in and get their Bayesian analysis done quickly, and explore the results with ease.  The following shows the rstan ecosystem.


```{r stan_getgraph, eval=TRUE, echo=FALSE}
stan_depgraph = get_depgraph('rstan')
```

```{r stan_graph, echo=F, eval=T, cache=FALSE}
nodes = stan_depgraph$node_df %>% 
  rename(label=name) %>% 
  rownames_to_column(var='id') %>% 
  mutate(id=as.integer(id)-1)
edges = stan_depgraph$edge_df %>% 
  rename(from=source,
         to=target)

listed_packs = c('brms', 'bayesplot', 'rstanarm', 'shinystan', 'loo', 'tidybayes', 'rethinking')
edges_trim = edges %>% 
  filter(to == 0) %>% 
  mutate(value = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 30L, value),
         length = if_else(from %in% nodes$id[nodes$label %in% listed_packs], 0, 250))

nodes_trim = nodes %>% 
  filter(id %in% c(0, edges_trim$from)) %>% 
  mutate(value=c(100, rep(0, nrow(.)-1)),
         value = if_else(label %in% listed_packs, 30, value))
  
visNetwork(nodes = nodes_trim, 
             edges=edges_trim) %>% 
  visEdges(color=list(opacity=.10),
           # length = c(200,10),
           physics = T)
```




To begin, one can write their own Stan modeling program and use <span class="pack">rstan</span> to run it.  The <span class="pack">rstanarm</span>, developed by the Stan group, makes it as easy to run glm, ordinal, mixed, and multivariate models as it is to run them with standard tools.  The <span class="pack">brms</span> package is ridiculously flexible, allowing one to conduct very complex models without every programming in the Stan language directly.  It has many distributional families, mixed modeling capability, and a variety of means to explore the model after the fact (some of which would apply to <span class="pack">rstanarm</span> as well).

```{r bayes_stan}
model = rstan::stan(file='mystandcode.stan', warmup = 1000, iter = 5000, thin = 10, cores=4)  # 4 chains in parallel
model = rstanarm::stan_glm(y ~ x, family = 'binomial')    # Bayesian GLM

# set priors for random effect and regression coefficients
priors = c(brms::prior(cauchy(0, 1), class = sd),
           prior(normal(0,10), class = b))

model = brm(y ~ x + (1 + x|group), family = 'zero_one_inflated_beta', prior = priors)   # lme4 style
summary(model)
launch_shinystan(model)                            # interactive exploration (also rstanarm)
pp_check(model)                                    # posterior predictive distribution (also rstanarm)
marginal_effects(model)                            # plot covariate effects
hypothesis(model, hypothesis = "exp(x) - 2 = 0")   # a specific test
```

Many other packages are using Stan under the hood as well for specific types of models.

## Other

Some newer packages to keep an eye on include <span class="pack">greta</span>, which looks to take the Hamiltonian Monte Carlo approach of Stan to Tensor Flow and big data, and <span class="pack">tidybayes</span>, which will make for some easier post-processing of model results.


## Bayesian list of packages

```{r bayes_percs, eval=TRUE, echo=FALSE}
rjags_perc = get_perc('rjags')
r2OpenBugs_perc = get_perc('r2OpenBugs')
MCMCpack_perc = get_perc('MCMCpack')
MCMCglmm_perc = get_perc('MCMCglmm')
coda_perc = get_perc('coda')
rstan_perc = get_perc('rstan')
rstanarm_perc = get_perc('rstanarm')
brms_perc = get_perc('brms')
greta_perc = get_perc('greta')
tidybayes_perc = get_perc('tidybayes')
```


- <span class="pack">rjags</span> `r rjags_perc`%
- <span class="pack">r2OpenBugs</span> `r r2OpenBugs_perc`%
- <span class="pack">MCMCpack</span> `r MCMCpack_perc`%
- <span class="pack">MCMCglmm</span> `r MCMCglmm_perc`%
- <span class="pack">coda</span> `r coda_perc`%
- <span class="pack">rstan</span> `r rstan_perc`%
- <span class="pack">rstanarm</span> `r rstanarm_perc`%
- <span class="pack">brms</span> `r brms_perc`%
- <span class="pack">greta</span> `r greta_perc`%
- <span class="pack">tidybayes</span>`r tidybayes_perc`%  Not on CRAN yet ([link](https://github.com/mjskay/tidybayes)).


## Bayesian recommended reading

[Bayesian Task View](https://www.rdocumentation.org/taskviews#Bayesian)

@kruschke_doing_2014  Bayesian puppies!

@mcelreath2016

@gelman_bda

You can learn more about Stan in my document: [Bayesian Basics](https://m-clark.github.io/bayesian-basics/).


# Text analysis

Most of text analysis involves serious data munging in order to take the unstructured text and put it into a form that's useful.  What analysis is done once the data is prepared isn't really specific to text.  For example, one might use any classification model for sentiment analysis, and latent dirichlet allocation can be applied to any matrix of counts.  In other instances, there are more text-oriented analyses, e.g. in the realm of deep learning (see keras).

The first thing to know is that there are many tools in the base R installation to deal with text.

```{r text_base_R}
grep(word_vec, pattern = 'xyz', value = T)          # return matching values
grepl(word_vec, pattern = 'xyz')                    # return a vector of TRUE, FALSE if match
gsub(word_vec, pattern='xyz', replacement = 'qt')   # string replacement
substr(word_vec, start = 5, stop = 10)              # substring
adist(word_vec1, word_vec2)                         # string distance between sets of words; see stringdist package
tolower(word_vec)                                   # convert to lower case
```

Generally you might want to use a package like <span class="pack">stringr</span>, which is actually a wrapper for both <span class="pack">stringi</span> and some base R functions.  This serves to mostly make functions work consistently, but there is little other advantage.

```{r text_stringr}
stringr::str_extract(word_vec, pattern = 'xyz')
str_replace(word_vec, pattern = 'xyz', replacement = 'qt')
str_remove(word_vec, pattern = 'xyz')
str_sub(word_vec, start = 5, end = 10)
```

When it comes to text analysis, there's a lot going on in the R world. Traditionally, there are packages like <span class="pack">tm</span>, <span class="pack">lda</span>, <span class="pack">koRpus</span>, and <span class="pack">topicmodels</span>.  More recently, there are ones that make things easier, faster, or add functionality. These include <span class="pack">tidytext</span>, <span class="pack">text2vec</span>, and <span class="pack">quanteda</span>.

The following assumes a tidy text data frame, where you have a column that notes the document, a column for the words in the document.  You can use basic <span class="pack">dplyr</span> functionality to remove stopwords and get word counts, then others to create the objects needed for further processing, and eventually analysis.  The following is an example of a topic modeling approach.

```{r text_easy}
lda_model = text2vec::LDA$new(n_topics = 10)   # topic model setup

processed_text %>% 
  anti_join(stop_words_df) %>%           # remove stopwords, with stop_words_df = data.frame(words=stop_word_vector) 
  count(document, words) %>%             # get word counts
  tidytext::cast_dfm() %>%               # create a document term/feature matrix
  quanteda::dfm_wordstem() %>%           # quanteda allows further processing while in dtm form
  text2vec::lda_model$fit_transform()    # fit a fast LDA
```

Examples of the intial processing can be found with [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) documentation, though folks very familiar with <span class="pack">dplyr</span> will see what's going on clearly.  In short, standard text processing in R is just about as easy as it can be.  Your biggest problem will be dealing with large amounts of text, for which R's memory hungry approach is not well suited.

As far as text-specific analysis goes, consider some packages like <span class="pack">stm</span>, <span class="pack">text2vec</span>, <span class="pack">quanteda</span>.

## Text analysis list of packages

```{r text_percs, eval=TRUE, echo=FALSE}
stringi_perc = get_perc('stringi')
stringr_perc = get_perc('stringr')
stringdist_perc = get_perc('stringdist')
tm_perc = get_perc('tm')
lda_perc = get_perc('lda')
topicmodels_perc = get_perc('topicmodels')
tidytext_perc = get_perc('tidytext')
quanteda_perc = get_perc('quanteda')
text2vec_perc = get_perc('text2vec')
stm_perc = get_perc('stm')
keras_perc = get_perc('keras')
```


- <span class="pack">stringi</span> `r stringi_perc`%
- <span class="pack">stringr</span> `r stringr_perc`%
- <span class="pack">stringdist</span> `r stringdist_perc`%
- <span class="pack">tm</span> `r tm_perc`%
- <span class="pack">lda</span> `r lda_perc`%
- <span class="pack">topicmodels</span> `r topicmodels_perc`%
- <span class="pack">tidytext</span> `r tidytext_perc`%
- <span class="pack">quanteda</span> `r quanteda_perc`%
- <span class="pack">text2vec</span> `r text2vec_perc`%
- <span class="pack">stm</span> `r stm_perc`%
- <span class="pack">keras</span>`r keras_perc`%  


## Text analysis recommended reading

[Task View](https://www.rdocumentation.org/taskviews#NaturalLanguageProcessing)

You can find detailed examples in my document [here](https://m-clark.github.io/text-analysis-with-R/).


# Discipline specific

Here are some discipline-specific task views that have not already been mentioned where you will find packages that may have just the modeling tools you're looking for.

- Ecological: [Task View](https://www.rdocumentation.org/taskviews#Environmetrics)
- Econometrics: [Task View](https://www.rdocumentation.org/taskviews#Econometrics)
- Financial: [Task View](https://www.rdocumentation.org/taskviews#Environmetrics)
- Genetics: [Task View](https://www.rdocumentation.org/taskviews#Genetics)
- Social Sciences: [Task View](https://www.rdocumentation.org/taskviews#SocialSciences)

# Modeling frameworks and helpers

Probably not enough to (consistently) do much here.

caret zelig recipes tm? sjstats car

# Misc to do


# References

[^optim]: Even the author of optim [doesn't recommend it](https://nashjc.wordpress.com/2016/11/10/why-optim-is-out-of-date/).

[^jagam]: There is the <span class="func">jagam</span> function in <span class="pack">mgcv</span> to use JAGS, but I can't think of a reason to use it over <span class="pack">brms</span>.

[^lgc]: Using a mixed model on longitudinal data with <span class="pack">flexmix</span> negates the need to use SEM for what are sometimes called <span class="emph">Latent Class Growth Curve Models</span>, <span class="emph">Growth Mixture Models</span> or <span class="emph">Latent Trajectory Models</span>.  If you need indirect effects, you'll have to use Mplus or similar, and good luck with that.

[^lavlack]: Currently <span class="pack">lavaan</span> lacks the incorporation of categorical latent variables (i.e. mixture models), but you have more functionality in R for those elsewhere than you would Mplus. The current <span class="pack">lavaan</span>  also lacks multilevel SEM, but that is a pretty rare model.  There are very minor other differences.

